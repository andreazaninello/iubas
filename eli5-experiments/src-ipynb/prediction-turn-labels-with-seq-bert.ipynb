{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '../../model_sequence_labeling/lib/src')\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"0\" # for debugging\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoConfig, AutoTokenizer, get_polynomial_decay_schedule_with_warmup\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import random\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from multi_turn_bert import MultiTurnBert\n",
    "from custom_dataset import CustomDataset, process_df\n",
    "import main as seq_labeling\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(ds_path):\n",
    "    df = pd.read_pickle(ds_path)\n",
    "\n",
    "    #Aligning the 5-levels labels to eli5 ones\n",
    "        \n",
    "    #'(D06) To answer - Other' -> '(D06) Answer - Other'\n",
    "    #'(D07) To provide agreement statement' -> '(D07) Agreement'\n",
    "    #'(D08) To provide disagreement statement' -> '(D08) Disagreement'\n",
    "    #'(D10) Other' -> '(D09) Other'\n",
    "    #'(D09) To provide informing statement' -> (D10) To provide informing statement\n",
    "    \n",
    "    \n",
    "    # (E10) Other -> (E09) Other \n",
    "    # (E09) Introducing Extraneous Information -> (E10) Introducing Extraneous Information\n",
    "    \n",
    "    df['exp_act_label'] = df.exp_act_label.apply(lambda x: '(E10) Other' if x == '(E09) Other' else x)\n",
    "    df['exp_act_label'] = df.exp_act_label.apply(lambda x: '(E09) Introducing Extraneous Information' if x == '(E10) Introducing Extraneous Information' else x)\n",
    "\n",
    "    df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D09) Other' if x == '(D10) Other' else x)\n",
    "    df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D10) To provide informing statement' if x == '(D09) To provide informing statement' else x)\n",
    "    \n",
    "    df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D06) Answer - Other' if x == '(D06) To answer - Other' else x)\n",
    "    df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D07) Agreement' if x == '(D07) To provide agreement statement' else x)\n",
    "    df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D08) Disagreement' if x == '(D08) To provide disagreement statement' else x)\n",
    "    \n",
    "\n",
    "    df['turn_text_with_topic'] = df.apply(lambda row: {\n",
    "                                        'author': row['turn_text']['author'], \n",
    "                                        'text'  : row['topic'].replace('_', ' ') + ' [SEP] ' +  row['turn_text']['text']\n",
    "                                       } ,axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train to predict explanation act:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is in src-py/trune_label_prediction_with_bert_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict and Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_dict = json.load(open('../data/topic_folds.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fivelvls_annotation_df = load_ds('../../data/five_levels_ds/annotation-results/MACE-measure/final_mace_predictions.pkl')\n",
    "eli5_annotation_df     = load_ds('../../data/eli5_ds/annotation-results/MACE-measure/final_mace_predictions_training.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fivelvls_annotation_df['ds'] = ['5lvls'] * len(fivelvls_annotation_df)\n",
    "eli5_annotation_df['ds'] = ['eli5'] * len(eli5_annotation_df)\n",
    "dlgs_df = pd.concat([fivelvls_annotation_df, eli5_annotation_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fold(fold_path, df, fold_idx, input_clm, output_clm, label_clm='exp_act_label', ds_names=[], model_name='bert-base-uncased'):\n",
    "    label_dictionary = {int(l[2:4])-1 : l for l in  df[label_clm].unique()}\n",
    "    \n",
    "    print(label_dictionary)\n",
    "    print(fold_path)\n",
    "    fold_topics = []\n",
    "    if 'eli5' in ds_names:\n",
    "        fold_topics += folds_dict['test']['eli5'][fold_idx]\n",
    "    if '5lvls' in ds_names:\n",
    "        fold_topics += folds_dict['test']['5lvls'][fold_idx]\n",
    "    \n",
    "    test_df  = df[df.topic.isin(fold_topics)]\n",
    "    \n",
    "    #Prepare the dataframe\n",
    "    grouped_df = test_df.groupby('task_id').agg({\n",
    "                                'turn_text': lambda rows: list(rows),\n",
    "                                 input_clm: lambda rows: list(rows),\n",
    "                                'topic': lambda rows: list(rows)[0],\n",
    "                                'topic_func_label': lambda rows: list(rows),\n",
    "                                'dlg_act_label': lambda rows: list(rows),\n",
    "                                'exp_act_label': lambda rows: list(rows)}).reset_index()\n",
    "\n",
    "    grouped_df['label'] = grouped_df[label_clm].apply(lambda labels: [int(x[2:4])-1 for x in labels]) #making labels parasable as integers\n",
    "\n",
    "    #Prepare the dataset\n",
    "    print(model_name)\n",
    "    tokenizer   = AutoTokenizer.from_pretrained(model_name)\n",
    "    bert_config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "    num_new_tokens = tokenizer.add_special_tokens(\n",
    "        {\n",
    "            'additional_special_tokens': [args.sp1_token, args.sp2_token, args.start_token]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    vocab=tokenizer.get_vocab()\n",
    "\n",
    "    args.vocab_size = len(vocab)\n",
    "    args.cls_token = tokenizer.cls_token\n",
    "    args.sep_token = tokenizer.sep_token\n",
    "    args.pad_token = tokenizer.pad_token\n",
    "    args.cls_id = vocab[args.cls_token]\n",
    "    args.sep_id = vocab[args.sep_token]\n",
    "    args.pad_id = vocab[args.pad_token]\n",
    "    args.sp1_id = vocab[args.sp1_token]\n",
    "    args.sp2_id = vocab[args.sp2_token]\n",
    "    args.start_id = vocab[args.start_token]\n",
    "    args.o_id   = -1\n",
    "    args.hidden_size = bert_config.hidden_size\n",
    "    args.p_dim = args.hidden_size\n",
    "    args.max_len = min(args.max_len, bert_config.max_position_embeddings)\n",
    "    args.num_classes=len(label_dictionary)\n",
    "    \n",
    "    test_ds = CustomDataset(grouped_df, tokenizer, input_clm, args, tokenizer.get_vocab())\n",
    "    eval_loader = DataLoader(test_ds, batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "\n",
    "    #Load the model\n",
    "    model = MultiTurnBert(args).to(args.device)\n",
    "    model.load_state_dict(torch.load(fold_path + 'best_model'))\n",
    "    model.eval()\n",
    "\n",
    "    eval_losses, eval_ys, eval_outputs, eval_lens = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(eval_loader)):\n",
    "            batch_x, batch_turns, batch_labels, true_turn_lens = batch\n",
    "\n",
    "            batch_x, batch_turns, batch_labels = \\\n",
    "                batch_x.to(args.device), batch_turns.to(args.device), batch_labels.to(args.device)\n",
    "\n",
    "            _, outputs = model(batch_x, args.pad_id, batch_turns, turns_labels=batch_labels)  # (), (B, L)\n",
    "\n",
    "            eval_ys.append(batch_labels.detach())\n",
    "            eval_outputs.append(outputs)\n",
    "            eval_lens.append(true_turn_lens)\n",
    "\n",
    "        eval_preds, eval_trues = [], []\n",
    "        for i in range(len(eval_ys)):\n",
    "            pred_batch, true_batch, batch_lens = eval_outputs[i], eval_ys[i], eval_lens[i]\n",
    "\n",
    "            batch_lens = batch_lens.tolist()  # (B)\n",
    "\n",
    "\n",
    "            pred_batch = [batch[:batch_lens[b]] for b, batch in enumerate(pred_batch)]\n",
    "            true_batch = [batch[:batch_lens[b]] for b, batch in enumerate(true_batch.tolist())]\n",
    "\n",
    "            assert len(pred_batch) == len(true_batch)\n",
    "            eval_preds += pred_batch\n",
    "            eval_trues += true_batch\n",
    "\n",
    "\n",
    "        #flatten preds and labels\n",
    "        eval_preds = [item[-1] for item in eval_preds]\n",
    "        eval_trues = [item[-1] for item in eval_trues]\n",
    "\n",
    "        #print(eval_preds)\n",
    "        #print(eval_trues)\n",
    "        score = f1_score(eval_trues, eval_preds, average ='macro')\n",
    "        test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n",
    "\n",
    "    return score, test_df\n",
    "\n",
    "def evaluate_model(models_path, label_clm, label_model, model_type='bert-base-uncased'):\n",
    "    f1_scores = {}\n",
    "\n",
    "    for model_name in ['eli5_models', '5lvls_models', 'all_models']:\n",
    "        model_scores = []\n",
    "        for ds_names in [['eli5'], ['5lvls'], ['5lvls', 'eli5']]:\n",
    "            temp_scores = []\n",
    "            for fold in range(5):\n",
    "                s, _ = evaluate_fold('{}/{}/{}/model/fold-{}/'.format(models_path, label_model, model_name, fold), dlgs_df.copy(), fold, 'turn_text_with_topic', label_clm + \"_predictions\", label_clm, ds_names=ds_names, model_name=model_type)\n",
    "                temp_scores.append(s)\n",
    "            \n",
    "            print(temp_scores)\n",
    "            s = np.mean(temp_scores)\n",
    "            model_scores.append(round(s, 2))\n",
    "\n",
    "        f1_scores[model_name] = model_scores\n",
    "    \n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = argparse.Namespace(turn_type='multi', pooling='cls', sp1_token='[EXPLAINER]', sp2_token='[EXPLAINEE]', \n",
    "                          bert_type='bert-base-uncased',\n",
    "                          max_len=256, max_turns=56, dropout=0.1, device='cuda', learning_rate=2e-5, warmup_ratio=0.01,\n",
    "                          batch_size=2, num_workers=2, num_epochs=5, num_classes=10, ckpt_dir='topic-func-cross-val-models', planning=False, start_token='[START]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len({9: '(D10) To provide informing statement', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 8: '(D09) Other', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{9: '(D10) To provide informing statement', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 8: '(D09) Other', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement'}\n",
      "../data/turn-label-models/seq/bert-base-uncased//dlg_act_label_prediction/eli5_models/model/fold-0/\n",
      "bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (725 > 512). Running this sequence through the model will result in indexing errors\n",
      "152it [00:00, 429.94it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 5155.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultiTurnBert:\n\tsize mismatch for position_wise_ff.weight: copying a param with shape torch.Size([11, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\n\tsize mismatch for position_wise_ff.bias: copying a param with shape torch.Size([11]) from checkpoint, the shape in current model is torch.Size([10]).\n\tsize mismatch for crf.start_transitions: copying a param with shape torch.Size([11]) from checkpoint, the shape in current model is torch.Size([10]).\n\tsize mismatch for crf.end_transitions: copying a param with shape torch.Size([11]) from checkpoint, the shape in current model is torch.Size([10]).\n\tsize mismatch for crf.transitions: copying a param with shape torch.Size([11, 11]) from checkpoint, the shape in current model is torch.Size([10, 10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e12d8a66414c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdlg_act_f1_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/turn-label-models/seq/bert-base-uncased/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dlg_act_label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dlg_act_label_prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-fa8d492307c8>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(models_path, label_clm, label_model, model_type)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mtemp_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}/{}/model/fold-{}/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdlgs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'turn_text_with_topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_clm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_predictions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_clm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mtemp_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-fa8d492307c8>\u001b[0m in \u001b[0;36mevaluate_fold\u001b[0;34m(fold_path, df, fold_idx, input_clm, output_clm, label_clm, ds_names, model_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiTurnBert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'best_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1498\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MultiTurnBert:\n\tsize mismatch for position_wise_ff.weight: copying a param with shape torch.Size([11, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\n\tsize mismatch for position_wise_ff.bias: copying a param with shape torch.Size([11]) from checkpoint, the shape in current model is torch.Size([10]).\n\tsize mismatch for crf.start_transitions: copying a param with shape torch.Size([11]) from checkpoint, the shape in current model is torch.Size([10]).\n\tsize mismatch for crf.end_transitions: copying a param with shape torch.Size([11]) from checkpoint, the shape in current model is torch.Size([10]).\n\tsize mismatch for crf.transitions: copying a param with shape torch.Size([11, 11]) from checkpoint, the shape in current model is torch.Size([10, 10])."
     ]
    }
   ],
   "source": [
    "dlg_act_f1_scores = evaluate_model('../data/turn-label-models/seq/bert-base-uncased/', 'dlg_act_label', 'dlg_act_label_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 8: '(E09) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 9: '(E10) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/turn-label-models/seq/bert-base-uncased//exp_act_label_prediction/eli5_ds_models/model/fold-0/\n",
      "bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (725 > 512). Running this sequence through the model will result in indexing errors\n",
      "152it [00:00, 328.05it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 4059.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/turn-label-models/seq/bert-base-uncased//exp_act_label_prediction/eli5_ds_models/model/fold-0/best_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1d6cc932f9b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp_act_f1_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/turn-label-models/seq/bert-base-uncased/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exp_act_label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exp_act_label_prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-711730830e47>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(models_path, label_clm, label_model, model_type)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mtemp_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}/{}/model/fold-{}/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdlgs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'turn_text_with_topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_clm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_predictions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_clm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mtemp_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-711730830e47>\u001b[0m in \u001b[0;36mevaluate_fold\u001b[0;34m(fold_path, df, fold_idx, input_clm, output_clm, label_clm, ds_names, model_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiTurnBert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'best_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/turn-label-models/seq/bert-base-uncased//exp_act_label_prediction/eli5_ds_models/model/fold-0/best_model'"
     ]
    }
   ],
   "source": [
    "exp_act_f1_scores = evaluate_model('../data/turn-label-models/seq/bert-base-uncased/', 'exp_act_label', 'exp_act_label_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/turn-label-models/seq/bert-base-uncased//topic_func_label_prediction/eli5_ds_models/model/fold-0/\n",
      "bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (725 > 512). Running this sequence through the model will result in indexing errors\n",
      "152it [00:00, 234.61it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 4009.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/turn-label-models/seq/bert-base-uncased//topic_func_label_prediction/eli5_ds_models/model/fold-0/best_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d12467e6a593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopic_func_f1_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/turn-label-models/seq/bert-base-uncased/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'topic_func_label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'topic_func_label_prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-711730830e47>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(models_path, label_clm, label_model, model_type)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mtemp_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}/{}/model/fold-{}/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdlgs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'turn_text_with_topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_clm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_predictions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_clm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mtemp_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-711730830e47>\u001b[0m in \u001b[0;36mevaluate_fold\u001b[0;34m(fold_path, df, fold_idx, input_clm, output_clm, label_clm, ds_names, model_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiTurnBert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'best_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/turn-label-models/seq/bert-base-uncased//topic_func_label_prediction/eli5_ds_models/model/fold-0/best_model'"
     ]
    }
   ],
   "source": [
    "topic_func_f1_scores = evaluate_model('../data/turn-label-models/seq/bert-base-uncased/', 'topic_func_label', 'topic_func_label_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach           ELI5    5lvls    ALL    ELI5    5lvls    ALL    ELI5    5lvls    ALL\n",
      "---------------  ------  -------  -----  ------  -------  -----  ------  -------  -----\n",
      "eli5_ds_models    0.4      0.198  0.223   0.361    0.233  0.351   0.308    0.324  0.342\n",
      "5lvls_ds_models   0.18     0.36   0.291   0.077    0.447  0.252   0.321    0.489  0.437\n",
      "mixed_ds_models   0.313    0.407  0.393   0.388    0.449  0.528   0.342    0.521  0.476\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([[x[0][0]] + x[0][1] + x[1][1] + x[2][1] for x in zip(exp_act_f1_scores.items(), dlg_act_f1_scores.items(), topic_func_f1_scores.items())], \n",
    "               headers=['Approach', 'ELI5', '5lvls', 'ALL', 'ELI5', '5lvls', 'ALL', 'ELI5', '5lvls', 'ALL']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on RoBERTa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(turn_type='multi', pooling='cls', sp1_token='[EXPLAINER]', sp2_token='[EXPLAINEE]', \n",
    "                          bert_type='roberta-base',\n",
    "                          max_len=256, max_turns=56, dropout=0.1, device='cuda', learning_rate=2e-5, warmup_ratio=0.01,\n",
    "                          batch_size=2, num_workers=2, num_epochs=5, num_classes=10, ckpt_dir='topic-func-cross-val-models', planning=False, start_token='[START]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "152it [00:00, 459.56it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 3821.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 746/746 [01:33<00:00,  7.95it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "79it [00:00, 539.64it/s]\n",
      "100%|██████████| 79/79 [00:00<00:00, 5477.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 288/288 [00:37<00:00,  7.65it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 657.85it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 6744.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 147/147 [00:20<00:00,  7.23it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "38it [00:00, 668.27it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 7391.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 120/120 [00:17<00:00,  6.99it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "33it [00:00, 685.64it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 6356.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 99/99 [00:14<00:00,  6.71it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32359499537389996, 0.37916671828237575, 0.4016324003647558, 0.32333396384041874, 0.3589307401447576]\n",
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 337.77it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 3065.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:21<00:00,  7.18it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 309.26it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2758.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 181/181 [00:24<00:00,  7.31it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 231.97it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2130.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 211/211 [00:27<00:00,  7.61it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 294.18it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2418.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:21<00:00,  7.13it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 358.44it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 3955.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 81/81 [00:12<00:00,  6.28it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13208065208065206, 0.2054864322077437, 0.1770565649963382, 0.15098340345127226, 0.2765186354131587]\n",
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "167it [00:00, 439.55it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 858.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 898/898 [01:52<00:00,  7.97it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 373.29it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "94it [00:00, 487.69it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 4271.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 468/468 [01:00<00:00,  7.79it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:00, 452.29it/s]\n",
      "100%|██████████| 58/58 [00:00<00:00, 3904.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 358/358 [00:46<00:00,  7.74it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "48it [00:00, 539.88it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 4964.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 272/272 [00:35<00:00,  7.59it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/eli5_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "43it [00:00, 584.80it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 5602.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 180/180 [00:24<00:00,  7.35it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31203317428574884, 0.3665927720375234, 0.3280634877139167, 0.2693765668871208, 0.3598428727484981]\n",
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "152it [00:00, 449.21it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 4534.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 746/746 [01:33<00:00,  7.94it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "79it [00:00, 526.26it/s]\n",
      "100%|██████████| 79/79 [00:00<00:00, 4766.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 288/288 [00:37<00:00,  7.64it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 656.94it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 6622.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 147/147 [00:20<00:00,  7.20it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "38it [00:00, 663.02it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 6499.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 120/120 [00:16<00:00,  7.11it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "33it [00:00, 682.78it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 7288.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 99/99 [00:14<00:00,  6.88it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13102986856244433, 0.07466073016287121, 0.12234784886994823, 0.0389520202020202, 0.09686607823907593]\n",
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 332.89it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2747.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:20<00:00,  7.26it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 305.99it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2845.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 181/181 [00:24<00:00,  7.31it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 211.29it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 1989.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 211/211 [00:36<00:00,  5.73it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 293.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2306.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:20<00:00,  7.25it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 365.20it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 4118.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 81/81 [00:12<00:00,  6.64it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5400946988949415, 0.47786202533684224, 0.5233718336102627, 0.3904670269764463, 0.43411432936571376]\n",
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "167it [00:00, 445.95it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 3575.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 898/898 [01:52<00:00,  7.96it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 375.48it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "94it [00:00, 492.23it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 4907.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 468/468 [01:00<00:00,  7.79it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:00, 453.25it/s]\n",
      "100%|██████████| 58/58 [00:00<00:00, 4329.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 358/358 [00:46<00:00,  7.71it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "48it [00:00, 518.86it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 4977.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 272/272 [00:35<00:00,  7.56it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/5lvls_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "43it [00:00, 595.05it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 5745.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 180/180 [00:24<00:00,  7.42it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1905767174013096, 0.2519838365400199, 0.32268176488593325, 0.2475948397751484, 0.2687994198249468]\n",
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "152it [00:00, 454.05it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 3632.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 746/746 [01:33<00:00,  7.96it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "79it [00:00, 539.64it/s]\n",
      "100%|██████████| 79/79 [00:00<00:00, 5699.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 288/288 [00:37<00:00,  7.59it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 662.36it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 6706.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 147/147 [00:20<00:00,  7.16it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "38it [00:00, 658.95it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 7126.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 120/120 [00:17<00:00,  6.93it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "33it [00:00, 685.40it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 7461.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 99/99 [00:14<00:00,  6.76it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3621405617430401, 0.4077391782799397, 0.4366038967849085, 0.3845589726825425, 0.3578229616692711]\n",
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 334.01it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 3015.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:20<00:00,  7.24it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 305.03it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2802.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 181/181 [00:24<00:00,  7.30it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 230.06it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2109.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 211/211 [00:28<00:00,  7.41it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 274.06it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2306.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:21<00:00,  7.23it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 365.04it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 4025.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 81/81 [00:12<00:00,  6.60it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4567997241491218, 0.5038336193755211, 0.5387784959279793, 0.45190392396274753, 0.38168038968424695]\n",
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "167it [00:00, 446.78it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 3327.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 898/898 [01:52<00:00,  7.97it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 376.82it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "94it [00:00, 490.89it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 4646.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 468/468 [00:59<00:00,  7.82it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:00, 455.93it/s]\n",
      "100%|██████████| 58/58 [00:00<00:00, 4384.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 358/358 [00:46<00:00,  7.70it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "48it [00:00, 527.94it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 4807.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 272/272 [00:35<00:00,  7.59it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: '(D09) Other', 6: '(D07) Agreement', 0: '(D01) To ask a check question', 4: '(D05) To answer a question by disconfirming', 1: '(D02) To ask what/how question', 5: '(D06) Answer - Other', 3: '(D04) To answer a question by confirming', 2: '(D03) To ask other kind of questions', 7: '(D08) Disagreement', 9: '(D10) To provide informing statement'}\n",
      "../data/roberta_seq/dlg_act_prediction/mixed_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "43it [00:00, 587.81it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 5521.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 180/180 [00:24<00:00,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4612762312262923, 0.5635712204328517, 0.5870294143890264, 0.5472888069272969, 0.4988352465054506]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    }
   ],
   "source": [
    "roberta_dlg_act_f1_scores = evaluate_model('../data/roberta_seq', 'dlg_act_label', 'dlg_act_prediction', model_type='roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "152it [00:00, 446.64it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 4598.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 746/746 [01:33<00:00,  7.94it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "79it [00:00, 545.17it/s]\n",
      "100%|██████████| 79/79 [00:00<00:00, 5589.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 288/288 [00:37<00:00,  7.62it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 659.05it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 5524.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 147/147 [00:20<00:00,  7.20it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "38it [00:00, 655.27it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 6203.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 120/120 [00:17<00:00,  6.95it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "33it [00:00, 680.06it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 7205.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 99/99 [00:14<00:00,  6.72it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2187595645640849, 0.414574612585882, 0.33978012105988453, 0.5031651564812817, 0.41500496031746037]\n",
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 334.74it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2886.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:20<00:00,  7.25it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 303.85it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2685.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 181/181 [00:24<00:00,  7.28it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 230.75it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 1827.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 211/211 [00:28<00:00,  7.34it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 282.05it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2348.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:21<00:00,  7.20it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 357.95it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 3835.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 81/81 [00:12<00:00,  6.48it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21161857408025833, 0.23374052595664108, 0.16383969708419072, 0.1841420910339937, 0.2223493516399695]\n",
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "167it [00:00, 440.17it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 3631.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 898/898 [01:52<00:00,  7.96it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 379.12it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "94it [00:00, 484.40it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 4917.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 468/468 [01:00<00:00,  7.80it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:00, 455.91it/s]\n",
      "100%|██████████| 58/58 [00:00<00:00, 3736.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 358/358 [00:46<00:00,  7.74it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "48it [00:00, 543.31it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 5182.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 272/272 [00:35<00:00,  7.58it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/eli5_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "43it [00:00, 588.65it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 5671.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 180/180 [00:24<00:00,  7.32it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2169531271936747, 0.30936915870953846, 0.22691383760174713, 0.22506034860407498, 0.23004697569687088]\n",
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "152it [00:00, 447.61it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 3930.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 746/746 [01:33<00:00,  7.96it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "79it [00:00, 539.86it/s]\n",
      "100%|██████████| 79/79 [00:00<00:00, 5732.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 288/288 [00:37<00:00,  7.63it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 646.84it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 6644.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 147/147 [00:20<00:00,  7.17it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "38it [00:00, 665.43it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 7301.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 120/120 [00:17<00:00,  7.05it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "33it [00:00, 670.62it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 7056.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 99/99 [00:14<00:00,  6.77it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14975468696250427, 0.1755573808549135, 0.18803534966325663, 0.1799578714935887, 0.14597386292301548]\n",
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 331.22it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 3006.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:21<00:00,  7.10it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 285.76it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 110.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 181/181 [00:24<00:00,  7.41it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 230.39it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2089.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 211/211 [00:28<00:00,  7.48it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 292.80it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2338.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:21<00:00,  7.02it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 325.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 3539.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 81/81 [00:11<00:00,  6.77it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39298986117338386, 0.30117360584237435, 0.3980069455491787, 0.3085166956662343, 0.32524096374751693]\n",
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "167it [00:00, 441.41it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 3582.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 898/898 [01:52<00:00,  7.99it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 371.65it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "94it [00:00, 486.94it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 4331.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 468/468 [00:59<00:00,  7.84it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:00, 457.28it/s]\n",
      "100%|██████████| 58/58 [00:00<00:00, 3599.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 358/358 [01:17<00:00,  4.62it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "48it [00:00, 528.38it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 4370.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 272/272 [00:39<00:00,  6.89it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/5lvls_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "43it [00:00, 580.58it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 5486.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 180/180 [00:28<00:00,  6.33it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2566598986141564, 0.2591168442083912, 0.3339441065171704, 0.2608837631480003, 0.23290261187758002]\n",
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "152it [00:00, 455.98it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 4106.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 746/746 [01:36<00:00,  7.70it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "79it [00:00, 540.96it/s]\n",
      "100%|██████████| 79/79 [00:00<00:00, 5711.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 288/288 [00:40<00:00,  7.04it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 659.12it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 6657.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 147/147 [00:23<00:00,  6.31it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "38it [00:00, 666.66it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 7321.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 120/120 [00:20<00:00,  5.84it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "33it [00:00, 676.46it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 7202.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 99/99 [00:17<00:00,  5.67it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27098945714460365, 0.33054817373023815, 0.29493283164869794, 0.36024527968097914, 0.3368336456571751]\n",
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 331.62it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2647.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:23<00:00,  6.35it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 302.99it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2672.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 181/181 [00:27<00:00,  6.49it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 230.94it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 1987.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 211/211 [00:31<00:00,  6.76it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 292.73it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2380.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:24<00:00,  6.25it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 360.08it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 4059.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 81/81 [00:15<00:00,  5.30it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3794652540442014, 0.3892702741445467, 0.4133671984955095, 0.3416565198443804, 0.34948313363682115]\n",
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "167it [00:00, 441.15it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 975.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 898/898 [01:56<00:00,  7.74it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:00, 377.73it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "94it [00:00, 489.81it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 3931.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 468/468 [01:03<00:00,  7.43it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:00, 455.40it/s]\n",
      "100%|██████████| 58/58 [00:00<00:00, 3855.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 358/358 [00:49<00:00,  7.28it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "48it [00:00, 537.48it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 4971.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 272/272 [00:39<00:00,  6.89it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '(E02) Testing prior knowledge', 2: '(E03) Provide an explanation', 4: '(E05) Signaling understanding', 9: '(E10) Introducing Extraneous Information', 6: '(E07) Providing Feedback', 0: '(E01) Testing understanding', 8: '(E09) Other', 3: '(E04) Ask for an explanation', 7: '(E08) Providing Assessment', 5: '(E06) Signaling non-understanding'}\n",
      "../data/roberta_seq/exp_act_prediction/mixed_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "43it [00:00, 593.29it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 5797.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 180/180 [00:27<00:00,  6.54it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3664889764968619, 0.40928403874468133, 0.4144258479832967, 0.35695628619216707, 0.35381534359099137]\n"
     ]
    }
   ],
   "source": [
    "roberta_exp_act_f1_scores = evaluate_model('../data/roberta_seq', 'exp_act_label', 'exp_act_prediction', model_type='roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "152it [00:00, 455.35it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 4082.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 746/746 [01:36<00:00,  7.69it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "79it [00:00, 542.72it/s]\n",
      "100%|██████████| 79/79 [00:00<00:00, 5675.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 288/288 [00:40<00:00,  7.14it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 655.30it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 6685.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 147/147 [00:23<00:00,  6.32it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "38it [00:00, 658.63it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 6987.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 120/120 [00:20<00:00,  5.99it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "33it [00:00, 673.80it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 7099.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 99/99 [00:18<00:00,  5.26it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29856779120970184, 0.37197290366732394, 0.33396556473829203, 0.33466810897303567, 0.33996336996336995]\n",
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 332.57it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 3061.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:24<00:00,  6.09it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 303.71it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2675.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 181/181 [00:27<00:00,  6.51it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 231.72it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2058.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 211/211 [00:30<00:00,  6.84it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 294.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 2394.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:24<00:00,  6.23it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 366.04it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 4180.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 81/81 [00:15<00:00,  5.37it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.353375435663567, 0.41308532145200577, 0.372874637552541, 0.4214784915919991, 0.3205540986242047]\n",
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "167it [00:00, 439.70it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 3658.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 468/468 [01:03<00:00,  7.43it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:00, 453.39it/s]\n",
      "100%|██████████| 58/58 [00:00<00:00, 4020.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 358/358 [00:49<00:00,  7.28it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "48it [00:00, 547.61it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 5096.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 272/272 [00:39<00:00,  6.80it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/eli5_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "43it [00:00, 593.21it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 5684.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 180/180 [00:27<00:00,  6.54it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3093932396914001, 0.40701560140974247, 0.37109360501828237, 0.4072645343951504, 0.3513629584622183]\n",
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/5lvls_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "152it [00:00, 447.85it/s]\n",
      "100%|██████████| 152/152 [00:00<00:00, 4601.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1492 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 746/746 [01:37<00:00,  7.68it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/5lvls_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n",
      "79it [00:00, 545.29it/s]\n",
      "100%|██████████| 79/79 [00:00<00:00, 4866.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 288/288 [00:41<00:00,  6.99it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/5lvls_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [00:00, 656.72it/s]\n",
      "100%|██████████| 43/43 [00:00<00:00, 6540.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 147/147 [00:28<00:00,  5.18it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/5lvls_ds_models/model/fold-3/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "38it [00:00, 661.18it/s]\n",
      "100%|██████████| 38/38 [00:00<00:00, 7080.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 120/120 [00:24<00:00,  4.93it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/5lvls_ds_models/model/fold-4/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (558 > 512). Running this sequence through the model will result in indexing errors\n",
      "33it [00:00, 677.11it/s]\n",
      "100%|██████████| 33/33 [00:00<00:00, 7242.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 99/99 [00:22<00:00,  4.36it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30076082232712276, 0.3408121973353778, 0.21136575842458194, 0.21352236394265478, 0.16615325940268733]\n",
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/5lvls_ds_models/model/fold-0/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 334.62it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2760.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 152/152 [00:24<00:00,  6.32it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/5lvls_ds_models/model/fold-1/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 304.34it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 2562.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 181/181 [00:27<00:00,  6.62it/s]\n",
      "<ipython-input-12-27a8ff436bcb>:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[output_clm] = [label_dictionary[x] for x in eval_preds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(T04) Other - No topic was introduced', 0: '(T01) It is the main topic', 2: '(T03) A related topic', 1: '(T02) A subtopic'}\n",
      "../data/roberta_seq/topic_func_prediction/5lvls_ds_models/model/fold-2/\n",
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 230.55it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 1983.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 samples are prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      " 70%|██████▉   | 147/211 [00:23<00:07,  8.15it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roberta_topic_func_f1_scores = evaluate_model('../data/roberta_seq', 'topic_func_label', 'topic_func_prediction', model_type='roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach           ELI5    5lvls    ALL    ELI5    5lvls    ALL    ELI5    5lvls    ALL\n",
      "---------------  ------  -------  -----  ------  -------  -----  ------  -------  -----\n",
      "eli5_ds_models    0.378    0.203  0.242   0.357    0.188  0.327   0.336    0.376  0.369\n",
      "5lvls_ds_models   0.168    0.345  0.269   0.093    0.473  0.256   0.247    0.549  0.408\n",
      "mixed_ds_models   0.319    0.375  0.38    0.39     0.467  0.532   0.371    0.553  0.494\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([[x[0][0]] + x[0][1] + x[1][1] + x[2][1] for x in zip(roberta_exp_act_f1_scores.items(), roberta_dlg_act_f1_scores.items(), roberta_topic_func_f1_scores.items())], \n",
    "               headers=['Approach', 'ELI5', '5lvls', 'ALL', 'ELI5', '5lvls', 'ALL', 'ELI5', '5lvls', 'ALL']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predicting labels for the final quality assessment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Performing Models:\n",
    "    - Explanation Moves: ELI-5 trained BERT-Seq\n",
    "    - Dialogue Acts: Both trained RoBERTa\n",
    "    - Topic Func: ELI-5 trained RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from multi_turn_bert import MultiTurnBert\n",
    "from custom_dataset import CustomDataset, process_df\n",
    "import main as seq_labeling\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5_annotation_df     = pd.read_pickle('../data/final_mace_predictions_testing_with_predictions.pkl') # This file was prepared in predicting-turn-labels-with-bert.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = argparse.Namespace(turn_type='multi', pooling='cls', sp1_token='[EXPLAINER]', sp2_token='[EXPLAINEE]', bert_type='bert-base-uncased',\n",
    "                          max_len=256, max_turns=56, dropout=0.1, device='cuda', learning_rate=2e-5, warmup_ratio=0.01,\n",
    "                          batch_size=2, num_workers=2, num_epochs=5, num_classes=-1, ckpt_dir='../data/seq-labeling/model', planning=False, start_token='[START]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predictions(model, eval_loader):\n",
    "    eval_losses, eval_ys, eval_outputs, eval_lens = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(eval_loader)):\n",
    "            batch_x, batch_turns, batch_labels, true_turn_lens = batch\n",
    "\n",
    "            batch_x, batch_turns, batch_labels = \\\n",
    "                batch_x.to(args.device), batch_turns.to(args.device), batch_labels.to(args.device)\n",
    "\n",
    "            _, outputs = model(batch_x, args.pad_id, batch_turns, turns_labels=batch_labels)  # (), (B, L)\n",
    "\n",
    "            eval_ys.append(batch_labels.detach())\n",
    "            eval_outputs.append(outputs)\n",
    "            eval_lens.append(true_turn_lens)\n",
    "\n",
    "\n",
    "        eval_preds, eval_trues = [], []\n",
    "        for i in range(len(eval_ys)):\n",
    "            pred_batch, true_batch, batch_lens = eval_outputs[i], eval_ys[i], eval_lens[i]\n",
    "\n",
    "            batch_lens = batch_lens.tolist()  # (B)\n",
    "\n",
    "\n",
    "            pred_batch = [batch[:batch_lens[b]] for b, batch in enumerate(pred_batch)]\n",
    "            true_batch = [batch[:batch_lens[b]] for b, batch in enumerate(true_batch.tolist())]\n",
    "\n",
    "            assert len(pred_batch) == len(true_batch)\n",
    "            eval_preds += pred_batch\n",
    "            eval_trues += true_batch\n",
    "\n",
    "\n",
    "        #flatten preds and labels\n",
    "        eval_preds = [item[-1] for item in eval_preds]\n",
    "        eval_trues = [item[-1] for item in eval_trues]\n",
    "\n",
    "        #print(eval_preds)\n",
    "        #print(eval_trues)\n",
    "        score = f1_score(eval_trues, eval_preds, average ='macro')\n",
    "        print('f1-score: {}'.format(score))\n",
    "        return eval_preds\n",
    "        \n",
    "def ensemble_generate(model_path, test_df, input_clm, args, label_clm='exp_act_label'):\n",
    "    \n",
    "        \n",
    "    label_dictionary = {int(l[2:4])-1 : l for l in  test_df[label_clm].unique()}\n",
    "    print(label_dictionary)\n",
    "        \n",
    "    #Prepare the dataframe\n",
    "    grouped_df = test_df.groupby('task_id').agg({\n",
    "                                'turn_text': lambda rows: list(rows),\n",
    "                                 input_clm: lambda rows: list(rows),\n",
    "                                'topic': lambda rows: list(rows)[0],\n",
    "                                'topic_func_label': lambda rows: list(rows),\n",
    "                                'dlg_act_label': lambda rows: list(rows),\n",
    "                                'exp_act_label': lambda rows: list(rows)}).reset_index()\n",
    "\n",
    "    grouped_df['label'] = grouped_df[label_clm].apply(lambda labels: [int(x[2:4])-1 for x in labels]) #making labels parasable as integers\n",
    "    \n",
    "    #Prepare the dataset\n",
    "    tokenizer   = AutoTokenizer.from_pretrained(args.bert_type)\n",
    "    bert_config = AutoConfig.from_pretrained(args.bert_type)\n",
    "\n",
    "    num_new_tokens = tokenizer.add_special_tokens(\n",
    "        {\n",
    "            'additional_special_tokens': [args.sp1_token, args.sp2_token, args.start_token]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    vocab=tokenizer.get_vocab()\n",
    "\n",
    "    args.vocab_size = len(vocab)\n",
    "    args.cls_token = tokenizer.cls_token\n",
    "    args.num_classes= len(label_dictionary)\n",
    "    args.sep_token = tokenizer.sep_token\n",
    "    args.pad_token = tokenizer.pad_token\n",
    "    args.cls_id = vocab[args.cls_token]\n",
    "    args.sep_id = vocab[args.sep_token]\n",
    "    args.pad_id = vocab[args.pad_token]\n",
    "    args.sp1_id = vocab[args.sp1_token]\n",
    "    args.sp2_id = vocab[args.sp2_token]\n",
    "    args.start_id = vocab[args.start_token]\n",
    "    args.o_id   = -1\n",
    "    args.hidden_size = bert_config.hidden_size\n",
    "    args.p_dim = args.hidden_size\n",
    "    args.max_len = min(args.max_len, bert_config.max_position_embeddings)\n",
    "    \n",
    "    test_ds = CustomDataset(grouped_df, tokenizer, input_clm, args, tokenizer.get_vocab())\n",
    "    eval_loader = DataLoader(test_ds, batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=True)\n",
    "\n",
    "    \n",
    "    models = []\n",
    "    for fold_path in glob(model_path):\n",
    "        #Load the model\n",
    "        print('Loading: {}'.format(fold_path))\n",
    "        args.ckpt_dir = fold_path\n",
    "        print(args)\n",
    "        model = MultiTurnBert(args).to(args.device)\n",
    "        model.load_state_dict(torch.load(fold_path + '/best_model'))\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    \n",
    "    models_preds = []\n",
    "    for model in models:\n",
    "        eval_loader = DataLoader(test_ds, batch_size=args.batch_size, num_workers=args.num_workers, pin_memory=True)\n",
    "        models_preds.append(model_predictions(model, eval_loader))\n",
    "\n",
    "    agg_predictions = [max(set(lst), key=lst.count) for lst in list(zip(*models_preds))] # [[1,2,..], [1,2,..]] -> [(1,1), (2,2),..] -> [1, 2]\n",
    "    \n",
    "    test_df[label_clm + '_predictions'] = [label_dictionary[x] for x in agg_predictions]\n",
    "    \n",
    "    print('Prec. {} Rec. {}, F1-score {}'.format(\n",
    "        precision_score(test_df['{}_predictions'.format(label_clm)], test_df[label_clm], average='macro'),\n",
    "        recall_score(test_df['{}_predictions'.format(label_clm)], test_df[label_clm], average='macro'),\n",
    "        f1_score(test_df['{}_predictions'.format(label_clm)], test_df[label_clm], average='macro')\n",
    "    ))\n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: '(E04) Ask for an explanation', 2: '(E03) Provide an explanation', 5: '(E06) Signaling non-understanding', 6: '(E07) Providing Feedback', 7: '(E08) Providing Assessment', 4: '(E05) Signaling understanding', 8: '(E09) Other', 9: '(E10) Introducing Extraneous Information', 0: '(E01) Testing understanding', 1: '(E02) Testing prior knowledge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (862 > 512). Running this sequence through the model will result in indexing errors\n",
      "122it [00:00, 405.05it/s]\n",
      "100%|██████████| 122/122 [00:00<00:00, 4303.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074 samples are prepared\n",
      "Loading: ../data/bert_seq/exp_act_prediction/eli5_ds_models/model/fold-4\n",
      "Namespace(batch_size=4, bert_type='bert-base-uncased', ckpt_dir='../data/bert_seq/exp_act_prediction/eli5_ds_models/model/fold-4', cls_id=101, cls_token='[CLS]', device='cuda', dropout=0.1, hidden_size=768, learning_rate=2e-05, max_len=256, max_turns=56, num_classes=10, num_epochs=5, num_workers=2, o_id=-1, p_dim=768, pad_id=0, pad_token='[PAD]', planning=False, pooling='cls', sep_id=102, sep_token='[SEP]', sp1_id=30522, sp1_token='[EXPLAINER]', sp2_id=30523, sp2_token='[EXPLAINEE]', start_id=30524, start_token='[START]', turn_type='multi', vocab_size=30525, warmup_ratio=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../data/bert_seq/exp_act_prediction/eli5_ds_models/model/fold-3\n",
      "Namespace(batch_size=4, bert_type='bert-base-uncased', ckpt_dir='../data/bert_seq/exp_act_prediction/eli5_ds_models/model/fold-3', cls_id=101, cls_token='[CLS]', device='cuda', dropout=0.1, hidden_size=768, learning_rate=2e-05, max_len=256, max_turns=56, num_classes=10, num_epochs=5, num_workers=2, o_id=-1, p_dim=768, pad_id=0, pad_token='[PAD]', planning=False, pooling='cls', sep_id=102, sep_token='[SEP]', sp1_id=30522, sp1_token='[EXPLAINER]', sp2_id=30523, sp2_token='[EXPLAINEE]', start_id=30524, start_token='[START]', turn_type='multi', vocab_size=30525, warmup_ratio=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../data/bert_seq/exp_act_prediction/eli5_ds_models/model/fold-1\n",
      "Namespace(batch_size=4, bert_type='bert-base-uncased', ckpt_dir='../data/bert_seq/exp_act_prediction/eli5_ds_models/model/fold-1', cls_id=101, cls_token='[CLS]', device='cuda', dropout=0.1, hidden_size=768, learning_rate=2e-05, max_len=256, max_turns=56, num_classes=10, num_epochs=5, num_workers=2, o_id=-1, p_dim=768, pad_id=0, pad_token='[PAD]', planning=False, pooling='cls', sep_id=102, sep_token='[SEP]', sp1_id=30522, sp1_token='[EXPLAINER]', sp2_id=30523, sp2_token='[EXPLAINEE]', start_id=30524, start_token='[START]', turn_type='multi', vocab_size=30525, warmup_ratio=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../data/bert_seq/exp_act_prediction/eli5_ds_models/model/fold-2\n",
      "Namespace(batch_size=4, bert_type='bert-base-uncased', ckpt_dir='../data/bert_seq/exp_act_prediction/eli5_ds_models/model/fold-2', cls_id=101, cls_token='[CLS]', device='cuda', dropout=0.1, hidden_size=768, learning_rate=2e-05, max_len=256, max_turns=56, num_classes=10, num_epochs=5, num_workers=2, o_id=-1, p_dim=768, pad_id=0, pad_token='[PAD]', planning=False, pooling='cls', sep_id=102, sep_token='[SEP]', sp1_id=30522, sp1_token='[EXPLAINER]', sp2_id=30523, sp2_token='[EXPLAINEE]', start_id=30524, start_token='[START]', turn_type='multi', vocab_size=30525, warmup_ratio=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ../data/bert_seq/exp_act_prediction/eli5_ds_models/model/fold-0\n",
      "Namespace(batch_size=4, bert_type='bert-base-uncased', ckpt_dir='../data/bert_seq/exp_act_prediction/eli5_ds_models/model/fold-0', cls_id=101, cls_token='[CLS]', device='cuda', dropout=0.1, hidden_size=768, learning_rate=2e-05, max_len=256, max_turns=56, num_classes=10, num_epochs=5, num_workers=2, o_id=-1, p_dim=768, pad_id=0, pad_token='[PAD]', planning=False, pooling='cls', sep_id=102, sep_token='[SEP]', sp1_id=30522, sp1_token='[EXPLAINER]', sp2_id=30523, sp2_token='[EXPLAINEE]', start_id=30524, start_token='[START]', turn_type='multi', vocab_size=30525, warmup_ratio=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 269/269 [01:00<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.24314582020547348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [01:00<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.23017451495581326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [01:00<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.26268251913542695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [01:00<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.22831691122341563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 269/269 [01:00<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.2190444661844179\n",
      "Prec. 0.24303831967278403 Rec. 0.22745843152311224, F1-score 0.23448281679263872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_df = ensemble_generate('../data/bert_seq/exp_act_prediction/eli5_ds_models/model/fold-*', eli5_annotation_df, 'turn_text_with_topic', args, label_clm='exp_act_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_pickle('../data/final_mace_predictions_testing_with_predictions.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
