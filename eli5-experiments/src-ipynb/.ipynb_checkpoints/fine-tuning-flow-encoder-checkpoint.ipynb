{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "sys.path.insert(0, '../src-py')\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#wandb.init(project=\"test-project\", entity=\"milad-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob \n",
    "from tabulate import tabulate\n",
    "import math\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from utils import *\n",
    "import quality_prediction_experiment as experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum seq num: 40\n",
      "Maximum seq len: 1099\n",
      "Data size: 345\n",
      "Maximum seq num: 40\n",
      "Maximum seq len: 1303\n",
      "Data size: 122\n"
     ]
    }
   ],
   "source": [
    "eli5_training_df= load_and_prepare_df('../../data/eli5_ds/annotation-results/MACE-measure/final_mace_predictions_training.pkl', \n",
    "                                      '../../data/eli5_ds/annotation-results/MACE-measure/final_mace_rating_predictions.csv')\n",
    "eli5_testing_df = load_and_prepare_df('../../data/eli5_ds/annotation-results/MACE-measure/final_mace_predictions_testing.pkl', \n",
    "                                      '../../data/eli5_ds/annotation-results/MACE-measure/final_mace_rating_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Finding best parameters for the flow-encoder:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuninig for exp_act:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### HatFormer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_153303-yyhk9bu6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/yyhk9bu6\" target=\"_blank\">hatformer-64-1-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca579dadc6447cc95619b717d54b49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f066aa51e620492784e94af9bd559c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bbfff70e8149f9ba2fd09e8b6cde75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-1-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-1-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78da496c3e604c7c9f61d4e360385c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.72739</td></tr><tr><td>eval/rmse</td><td>1.3143</td></tr><tr><td>eval/runtime</td><td>0.6256</td></tr><tr><td>eval/samples_per_second</td><td>105.49</td></tr><tr><td>eval/steps_per_second</td><td>27.172</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1892516699382528.0</td></tr><tr><td>train/train_loss</td><td>1.66864</td></tr><tr><td>train/train_runtime</td><td>86.0077</td></tr><tr><td>train/train_samples_per_second</td><td>21.219</td></tr><tr><td>train/train_steps_per_second</td><td>5.348</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-64-1-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/yyhk9bu6\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/yyhk9bu6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_153303-yyhk9bu6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_153445-3nm88hfs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3nm88hfs\" target=\"_blank\">hatformer-64-1-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df75d26041bf4045b38df67e111a456c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be0a26c08a649e1ac4eaff01c9752e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda53761bb8e4e699e43d05b22b74891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-1-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-1-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0389ec6df244a19b7d0b0e6d62a51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.13175</td></tr><tr><td>eval/rmse</td><td>1.46005</td></tr><tr><td>eval/runtime</td><td>0.6984</td></tr><tr><td>eval/samples_per_second</td><td>94.507</td></tr><tr><td>eval/steps_per_second</td><td>24.343</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1893771667022592.0</td></tr><tr><td>train/train_loss</td><td>1.22509</td></tr><tr><td>train/train_runtime</td><td>90.1097</td></tr><tr><td>train/train_samples_per_second</td><td>20.253</td></tr><tr><td>train/train_steps_per_second</td><td>5.105</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-64-1-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3nm88hfs\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3nm88hfs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_153445-3nm88hfs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_153630-hqmq8umo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/hqmq8umo\" target=\"_blank\">hatformer-64-1-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df19c277d8cc4d01930e1924d832e06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee54f617c6ce42019aa1b45fd747631e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d116eeead7b14a46a3d8ccdf05c1aab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-1-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-1-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87c1f8f811f442eab546bc186307b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.42962</td></tr><tr><td>eval/rmse</td><td>1.55872</td></tr><tr><td>eval/runtime</td><td>0.7283</td></tr><tr><td>eval/samples_per_second</td><td>90.626</td></tr><tr><td>eval/steps_per_second</td><td>23.343</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896281602302720.0</td></tr><tr><td>train/train_loss</td><td>1.22343</td></tr><tr><td>train/train_runtime</td><td>104.206</td></tr><tr><td>train/train_samples_per_second</td><td>17.513</td></tr><tr><td>train/train_steps_per_second</td><td>4.414</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-64-1-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/hqmq8umo\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/hqmq8umo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_153630-hqmq8umo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 4, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_153830-23ehae81</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/23ehae81\" target=\"_blank\">hatformer-64-4-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4894affac7b4c9abc18c3c01898f881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8869355b401b4249b88332283fbf9aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5e9aeddf3b4fb7acfcf1bc93bdb447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-4-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-4-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331404f3e46847cabeaf8a3eeaeb303a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.92941</td></tr><tr><td>eval/rmse</td><td>1.38903</td></tr><tr><td>eval/runtime</td><td>0.651</td></tr><tr><td>eval/samples_per_second</td><td>101.377</td></tr><tr><td>eval/steps_per_second</td><td>26.112</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1892516699382528.0</td></tr><tr><td>train/train_loss</td><td>1.3057</td></tr><tr><td>train/train_runtime</td><td>86.2256</td></tr><tr><td>train/train_samples_per_second</td><td>21.165</td></tr><tr><td>train/train_steps_per_second</td><td>5.335</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-64-4-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/23ehae81\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/23ehae81</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_153830-23ehae81/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 4, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_154011-3qhmteul</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3qhmteul\" target=\"_blank\">hatformer-64-4-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb4d991fc294099b314049628d81bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f129ff60cae4ba39af76861b6442567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b1844b2a074d00bb51fa0427eabb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-4-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-4-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c20e02da7cd45f2b351cbaf63236ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.16687</td></tr><tr><td>eval/rmse</td><td>1.47203</td></tr><tr><td>eval/runtime</td><td>0.7265</td></tr><tr><td>eval/samples_per_second</td><td>90.842</td></tr><tr><td>eval/steps_per_second</td><td>23.399</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1893771667022592.0</td></tr><tr><td>train/train_loss</td><td>1.22929</td></tr><tr><td>train/train_runtime</td><td>90.8113</td></tr><tr><td>train/train_samples_per_second</td><td>20.097</td></tr><tr><td>train/train_steps_per_second</td><td>5.065</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-64-4-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3qhmteul\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3qhmteul</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_154011-3qhmteul/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 4, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_154156-kvrgpvhb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/kvrgpvhb\" target=\"_blank\">hatformer-64-4-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9465ebd426c4497e90827227f8cc048a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f33425d6764e25a6c0c0b38d3c282b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47a8c4c127e41d79e9e11895fc6a0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-4-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-4-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715da6960e69421da3d9f0aa0bdac99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.21925</td></tr><tr><td>eval/rmse</td><td>1.48972</td></tr><tr><td>eval/runtime</td><td>0.9115</td></tr><tr><td>eval/samples_per_second</td><td>72.412</td></tr><tr><td>eval/steps_per_second</td><td>18.651</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896281602302720.0</td></tr><tr><td>train/train_loss</td><td>1.22182</td></tr><tr><td>train/train_runtime</td><td>104.4833</td></tr><tr><td>train/train_samples_per_second</td><td>17.467</td></tr><tr><td>train/train_steps_per_second</td><td>4.403</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-64-4-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/kvrgpvhb\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/kvrgpvhb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_154156-kvrgpvhb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 8, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_154356-pd8hift5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/pd8hift5\" target=\"_blank\">hatformer-64-8-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2323ee2649e4014a9e180e91924dac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08049314be954a799a4a6d0977bbf75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b341a54d45470ab18cdf041125f6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-8-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-8-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d4f04751a24958bc2d655766a8e533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.00533</td></tr><tr><td>eval/rmse</td><td>1.4161</td></tr><tr><td>eval/runtime</td><td>0.6115</td></tr><tr><td>eval/samples_per_second</td><td>107.923</td></tr><tr><td>eval/steps_per_second</td><td>27.798</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1892516699382528.0</td></tr><tr><td>train/train_loss</td><td>1.18174</td></tr><tr><td>train/train_runtime</td><td>85.7443</td></tr><tr><td>train/train_samples_per_second</td><td>21.284</td></tr><tr><td>train/train_steps_per_second</td><td>5.365</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-64-8-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/pd8hift5\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/pd8hift5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_154356-pd8hift5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 8, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_154537-1rnfp2w8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1rnfp2w8\" target=\"_blank\">hatformer-64-8-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9539ada75f4594b80ea5b80a95c121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f68452242e4442c9d0223af9a7107db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b864590f6a348b9aead2d9f35087928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-8-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-8-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10de6fa680ef409a8416e11ca72793ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.9186</td></tr><tr><td>eval/rmse</td><td>1.38513</td></tr><tr><td>eval/runtime</td><td>0.6466</td></tr><tr><td>eval/samples_per_second</td><td>102.074</td></tr><tr><td>eval/steps_per_second</td><td>26.292</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1893771667022592.0</td></tr><tr><td>train/train_loss</td><td>1.20846</td></tr><tr><td>train/train_runtime</td><td>90.4142</td></tr><tr><td>train/train_samples_per_second</td><td>20.185</td></tr><tr><td>train/train_steps_per_second</td><td>5.088</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-64-8-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/1rnfp2w8\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/1rnfp2w8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_154537-1rnfp2w8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 8, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_154722-kimwxjl1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/kimwxjl1\" target=\"_blank\">hatformer-64-8-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad89d6be98fb410c9fe26c0d57a10174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bb921213e747ef805d4555c9cf3cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652c32a246664b0d9deab7ecc4d6dbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-8-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-64-8-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f693b7ed8e73410ab5c837ac43bac021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.26989</td></tr><tr><td>eval/rmse</td><td>1.50661</td></tr><tr><td>eval/runtime</td><td>0.6986</td></tr><tr><td>eval/samples_per_second</td><td>94.472</td></tr><tr><td>eval/steps_per_second</td><td>24.334</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896281602302720.0</td></tr><tr><td>train/train_loss</td><td>1.28923</td></tr><tr><td>train/train_runtime</td><td>103.1337</td></tr><tr><td>train/train_samples_per_second</td><td>17.695</td></tr><tr><td>train/train_steps_per_second</td><td>4.46</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-64-8-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/kimwxjl1\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/kimwxjl1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_154722-kimwxjl1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 1, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_154920-1fhvcz4b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1fhvcz4b\" target=\"_blank\">hatformer-128-1-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ada26e46bf849f8a55aa8334d7a8d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce412d6a65c24db28bd71acf29bf7a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72c82d8156d4fa39047edfd1f1eef07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-1-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-1-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe775a647c84675b47d3f78df3f5805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.89981</td></tr><tr><td>eval/rmse</td><td>1.37834</td></tr><tr><td>eval/runtime</td><td>0.6481</td></tr><tr><td>eval/samples_per_second</td><td>101.843</td></tr><tr><td>eval/steps_per_second</td><td>26.232</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896423874437888.0</td></tr><tr><td>train/train_loss</td><td>1.4378</td></tr><tr><td>train/train_runtime</td><td>86.4041</td></tr><tr><td>train/train_samples_per_second</td><td>21.122</td></tr><tr><td>train/train_steps_per_second</td><td>5.324</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-128-1-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/1fhvcz4b\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/1fhvcz4b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_154920-1fhvcz4b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 1, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_155102-18bay4q7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/18bay4q7\" target=\"_blank\">hatformer-128-1-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b89218a58034f12a795f43e0a66c4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9562a3dd5d4dd981a6680058783775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2f6ddf48104503acc1b6967a817d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-1-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-1-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d757b71245134857be52746b381b1250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.68178</td></tr><tr><td>eval/rmse</td><td>1.29683</td></tr><tr><td>eval/runtime</td><td>0.67</td></tr><tr><td>eval/samples_per_second</td><td>98.5</td></tr><tr><td>eval/steps_per_second</td><td>25.371</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1901380041057024.0</td></tr><tr><td>train/train_loss</td><td>1.18795</td></tr><tr><td>train/train_runtime</td><td>90.858</td></tr><tr><td>train/train_samples_per_second</td><td>20.086</td></tr><tr><td>train/train_steps_per_second</td><td>5.063</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-128-1-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/18bay4q7\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/18bay4q7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_155102-18bay4q7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 1, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_155247-13rlczey</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/13rlczey\" target=\"_blank\">hatformer-128-1-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b097729d3444ce5ba9f0554def82948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdafdb6339a84d849def501ec0df4b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a37391499614bcca096b178c47f5021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-1-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-1-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e578001f4e4aef899a03a418060963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.76243</td></tr><tr><td>eval/rmse</td><td>1.32757</td></tr><tr><td>eval/runtime</td><td>0.68</td></tr><tr><td>eval/samples_per_second</td><td>97.063</td></tr><tr><td>eval/steps_per_second</td><td>25.001</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911292374295296.0</td></tr><tr><td>train/train_loss</td><td>1.32978</td></tr><tr><td>train/train_runtime</td><td>101.5116</td></tr><tr><td>train/train_samples_per_second</td><td>17.978</td></tr><tr><td>train/train_steps_per_second</td><td>4.532</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-128-1-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/13rlczey\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/13rlczey</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_155247-13rlczey/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 4, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_155443-1bpqda0r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1bpqda0r\" target=\"_blank\">hatformer-128-4-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8a70e7447f46ab85734efb7a9319b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a87f95094149ba912b15a33c550f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26700dc16e474db9af08c4b0d23cb60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-4-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-4-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4c5511fc764176a8a4dce3b3d8d27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.86564</td></tr><tr><td>eval/rmse</td><td>1.36589</td></tr><tr><td>eval/runtime</td><td>0.6057</td></tr><tr><td>eval/samples_per_second</td><td>108.973</td></tr><tr><td>eval/steps_per_second</td><td>28.069</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896423874437888.0</td></tr><tr><td>train/train_loss</td><td>1.22736</td></tr><tr><td>train/train_runtime</td><td>86.2551</td></tr><tr><td>train/train_samples_per_second</td><td>21.158</td></tr><tr><td>train/train_steps_per_second</td><td>5.333</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-128-4-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/1bpqda0r\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/1bpqda0r</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_155443-1bpqda0r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 4, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_155625-3ltgxzzo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3ltgxzzo\" target=\"_blank\">hatformer-128-4-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296d492ec2544e77b2eb6d082e32193c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0172616391f40338d397afdbf0963d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2911ef01fef6455689d46849508bea1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-4-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-4-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7f88d23e8948dd9c63836b764d753b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.00998</td></tr><tr><td>eval/rmse</td><td>1.41774</td></tr><tr><td>eval/runtime</td><td>0.68</td></tr><tr><td>eval/samples_per_second</td><td>97.064</td></tr><tr><td>eval/steps_per_second</td><td>25.001</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1901380041057024.0</td></tr><tr><td>train/train_loss</td><td>1.2474</td></tr><tr><td>train/train_runtime</td><td>91.5776</td></tr><tr><td>train/train_samples_per_second</td><td>19.928</td></tr><tr><td>train/train_steps_per_second</td><td>5.023</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-128-4-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3ltgxzzo\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3ltgxzzo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_155625-3ltgxzzo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 4, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_155820-3m98hd2x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3m98hd2x\" target=\"_blank\">hatformer-128-4-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de40b62558c7427b940c373ce426b676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf3b22b79b44714bc3240342221f798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d414950b7be4723b5ba33cbc714142b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-4-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-4-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3208c1e10a21451f8bab15f18a4ed863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.66553</td></tr><tr><td>eval/rmse</td><td>1.29056</td></tr><tr><td>eval/runtime</td><td>0.7561</td></tr><tr><td>eval/samples_per_second</td><td>87.292</td></tr><tr><td>eval/steps_per_second</td><td>22.484</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911292374295296.0</td></tr><tr><td>train/train_loss</td><td>1.25718</td></tr><tr><td>train/train_runtime</td><td>105.2151</td></tr><tr><td>train/train_samples_per_second</td><td>17.345</td></tr><tr><td>train/train_steps_per_second</td><td>4.372</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-128-4-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3m98hd2x\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3m98hd2x</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_155820-3m98hd2x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 8, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_160020-3oi0r8ee</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3oi0r8ee\" target=\"_blank\">hatformer-128-8-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144a41a21bc7499e85bcd6bd271e633b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da53f4cd93a4996844485165003cc35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801298dbe4c743f1a2b6b0d5e4c52b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-8-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-8-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec03c8e394245469527331f339346d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.91112</td></tr><tr><td>eval/rmse</td><td>1.38243</td></tr><tr><td>eval/runtime</td><td>0.5975</td></tr><tr><td>eval/samples_per_second</td><td>110.461</td></tr><tr><td>eval/steps_per_second</td><td>28.452</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896423874437888.0</td></tr><tr><td>train/train_loss</td><td>1.33183</td></tr><tr><td>train/train_runtime</td><td>85.7362</td></tr><tr><td>train/train_samples_per_second</td><td>21.286</td></tr><tr><td>train/train_steps_per_second</td><td>5.365</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-128-8-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3oi0r8ee\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3oi0r8ee</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_160020-3oi0r8ee/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 8, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_160200-r8luv207</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/r8luv207\" target=\"_blank\">hatformer-128-8-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc02ddc0d89409b91248c3069d99b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59b54fc60b84b3aaadd275f5f7ceb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f410c4d39046dd9f1f88bd6f3458e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, __index_level_0__, input_texts. If topic, exp_act_label, __index_level_0__, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-8-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/exp-act-fine-tuning//hatformer-128-8-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad4a9bbe3064621ae7fe927dbb57ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.30348</td></tr><tr><td>eval/rmse</td><td>1.51772</td></tr><tr><td>eval/runtime</td><td>0.6707</td></tr><tr><td>eval/samples_per_second</td><td>98.398</td></tr><tr><td>eval/steps_per_second</td><td>25.345</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1901380041057024.0</td></tr><tr><td>train/train_loss</td><td>1.26728</td></tr><tr><td>train/train_runtime</td><td>91.1755</td></tr><tr><td>train/train_samples_per_second</td><td>20.016</td></tr><tr><td>train/train_steps_per_second</td><td>5.045</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hatformer-128-8-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/r8luv207\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/r8luv207</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_160200-r8luv207/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 8, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'classifier.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_160346-8v0tgqbu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/8v0tgqbu\" target=\"_blank\">hatformer-128-8-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e402e4673e804f42bb37c2087025ed57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81112d5247134206a7593b54ea9b4b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e49cea7ba3488fa30ca5e8e7073baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, exp_act_label, input_texts. If topic, exp_act_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='217' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [217/460 00:49 < 00:55, 4.35 it/s, Epoch 2.35/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [ 3, 6, 12]:\n",
    "            experiments.train_and_evaluate_single_fold('hatformer', \n",
    "                   '../data/fine-tuning/exp-act-fine-tuning/', \n",
    "                   eli5_training_df[['topic', 'input_texts', 'exp_act_label', 'labels']], \n",
    "                   'hatformer-{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), \n",
    "                   extra_encoder_configs=[{'num_tokens':11, 'flow_model_hidden_size': flow_model_hidden_size, 'nhead': nhead, 'nlayers':nlayers}],\n",
    "                   lr=2e-5, batch_size=4,\n",
    "                   model_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 128-8-12 : 1.2740188837051392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64-1-3 : 1.3143020868301392\n",
      "64-1-6 : 1.4600518941879272\n",
      "64-1-12 : 1.5587223768234253\n",
      "64-4-3 : 1.3890334367752075\n",
      "64-4-6 : 1.472029447555542\n",
      "64-4-12 : 1.4897152185440063\n",
      "64-8-3 : 1.4160959720611572\n",
      "64-8-6 : 1.3851345777511597\n",
      "64-8-12 : 1.5066144466400146\n",
      "128-1-3 : 1.3783365488052368\n",
      "128-1-6 : 1.2968330383300781\n",
      "128-1-12 : 1.3275666236877441\n",
      "128-4-3 : 1.3658860921859741\n",
      "128-4-6 : 1.4177370071411133\n",
      "128-4-12 : 1.2905558347702026\n",
      "128-8-3 : 1.382430911064148\n",
      "128-8-6 : 1.5177234411239624\n",
      "128-8-12 : 1.2740188837051392\n",
      "256-1-3 : 1.5305010080337524\n",
      "256-1-6 : 1.5499168634414673\n",
      "256-1-12 : 1.4402225017547607\n",
      "256-4-3 : 1.4296478033065796\n",
      "256-4-6 : 1.6064833402633667\n",
      "256-4-12 : 1.4927970170974731\n",
      "256-8-3 : 1.4401403665542603\n",
      "256-8-6 : 1.4269355535507202\n",
      "256-8-12 : 1.4212628602981567\n"
     ]
    }
   ],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [3, 6, 12]:\n",
    "            res = json.load(open('../data/fine-tuning/exp-act-fine-tuning/hatformer-{}-{}-{}/0-fold/eval_results.json'.format(flow_model_hidden_size, nhead, nlayers, flow_model_hidden_size, nhead, nlayers)))\n",
    "            print('{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), ':' , res['eval_rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Longformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: allenai/longformer-base-4096\n",
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'classifier.dense.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'classifier.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'classifier.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiladalsh-it\u001b[0m (\u001b[33mmilad-it\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_213959-i6zt729v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/i6zt729v\" target=\"_blank\">model-64-1-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input_clm=input_texts\n",
      "adding exp_act_label to the flows\n",
      "Using input_clm=input_texts\n",
      "adding exp_act_label to the flows\n",
      "Using input_clm=input_texts\n",
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 08:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/longformer/exp-act-fine-tuning//model-64-1-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/longformer/exp-act-fine-tuning//model-64-1-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4861543d950c4053808a03a5672d85a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.55778</td></tr><tr><td>eval/rmse</td><td>1.24811</td></tr><tr><td>eval/runtime</td><td>4.1969</td></tr><tr><td>eval/samples_per_second</td><td>15.726</td></tr><tr><td>eval/steps_per_second</td><td>4.051</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>4829649022771200.0</td></tr><tr><td>train/train_loss</td><td>1.51429</td></tr><tr><td>train/train_runtime</td><td>515.0434</td></tr><tr><td>train/train_samples_per_second</td><td>3.543</td></tr><tr><td>train/train_steps_per_second</td><td>0.893</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-1-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/i6zt729v\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/i6zt729v</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_213959-i6zt729v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: allenai/longformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/859f4633944e1b7e7fa301e72161388cd5903e36385d0ef2917256506bff64c3.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/af6fcabe2bf8cab6f77b20d94ba46a3dbf441ca0549e1f3c852c437b612f5224.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
      "Model config LongformerConfig {\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"onnx_export\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/allenai/longformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/a7a586602e625bd012d75abdfcc615f5bb1fe133273845f7381332c634273bd9.dc3a4f03d4ab11f972b126d0e6b67f43e5d9003b3aec54f8e549cc7e2d42398d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'classifier.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'classifier.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'classifier.dense.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'classifier.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_214853-o7teb3bl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/o7teb3bl\" target=\"_blank\">model-64-1-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input_clm=input_texts\n",
      "adding exp_act_label to the flows\n",
      "Using input_clm=input_texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "Using input_clm=input_texts\n",
      "adding exp_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 87/460 01:36 < 07:02, 0.88 it/s, Epoch 0.93/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [3, 6, 12]:\n",
    "            \n",
    "            experiments.train_and_evaluate_single_fold('longformer', \n",
    "                   '../data/fine-tuning/longformer/exp-act-fine-tuning/', \n",
    "                   eli5_training_df[['topic', 'input_texts', 'exp_act_label', 'labels']], 'model-{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), \n",
    "                   extra_encoder_configs=[{'num_tokens':11, 'flow_model_hidden_size': flow_model_hidden_size, 'nhead': nhead, 'nlayers':nlayers}],\n",
    "                   lr=2e-5, batch_size=4,model_path=None, input_clm=\"input_texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64-1-3 : 1.2481111288070679\n",
      "64-1-6 : 1.0265811681747437\n",
      "64-1-12 : 1.2468509674072266\n",
      "64-4-3 : 1.1406197547912598\n",
      "64-4-6 : 1.1189788579940796\n",
      "64-4-12 : 1.1625596284866333\n",
      "64-8-3 : 1.1211936473846436\n",
      "64-8-6 : 1.1348460912704468\n",
      "64-8-12 : 1.2308238744735718\n",
      "128-1-3 : 1.200014591217041\n",
      "128-1-6 : 1.0938385725021362\n",
      "128-1-12 : 1.0944148302078247\n",
      "128-4-3 : 1.1719892024993896\n",
      "128-4-6 : 1.0629855394363403\n",
      "128-4-12 : 1.1136494874954224\n",
      "128-8-3 : 1.1963645219802856\n",
      "128-8-6 : 1.0578935146331787\n",
      "128-8-12 : 1.0914322137832642\n",
      "256-1-3 : 1.3572139739990234\n",
      "256-1-6 : 1.3090100288391113\n",
      "256-1-12 : 1.242100715637207\n",
      "256-4-3 : 1.113281488418579\n",
      "256-4-6 : 1.161912441253662\n",
      "256-4-12 : 1.1854363679885864\n",
      "256-8-3 : 1.3244274854660034\n",
      "256-8-6 : 1.2449488639831543\n",
      "256-8-12 : 1.1539956331253052\n"
     ]
    }
   ],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [3, 6, 12]:\n",
    "            res = json.load(open('../data/fine-tuning/longformer/exp-act-fine-tuning/model-{}-{}-{}/0-fold/eval_results.json'.format(flow_model_hidden_size, nhead, nlayers, flow_model_hidden_size, nhead, nlayers)))\n",
    "            print('{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), ':' , res['eval_rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning for dlg_act:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n",
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiladalsh-it\u001b[0m (\u001b[33mmilad-it\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_171114-3comsoaq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3comsoaq\" target=\"_blank\">model-64-1-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function train_and_evaluate_single_fold.<locals>.<lambda> at 0x7f66d82c0790> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3608fed7ffef4fe9822fa25b85b52262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c850d75fef4187bfc631b0568bef90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240344e0824b4419977774f2761cd5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-1-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-1-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db42ba327bdb4feca556e8777ac5531a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.0106</td></tr><tr><td>eval/rmse</td><td>1.41796</td></tr><tr><td>eval/runtime</td><td>0.7396</td></tr><tr><td>eval/samples_per_second</td><td>89.232</td></tr><tr><td>eval/steps_per_second</td><td>22.984</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1892516699382528.0</td></tr><tr><td>train/train_loss</td><td>1.48138</td></tr><tr><td>train/train_runtime</td><td>97.0166</td></tr><tr><td>train/train_samples_per_second</td><td>18.811</td></tr><tr><td>train/train_steps_per_second</td><td>4.741</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-1-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3comsoaq\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3comsoaq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_171114-3comsoaq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_171307-14l7q6pj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/14l7q6pj\" target=\"_blank\">model-64-1-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcd1c2c4f944f249239f37a4456c6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10b3bfb470840b0a1ea238c37ae6b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcbf97e8ee146b0b5c0263ce2a8abac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-1-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-1-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e010dce1dca6497988da75b4f4672303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.96396</td></tr><tr><td>eval/rmse</td><td>1.40141</td></tr><tr><td>eval/runtime</td><td>0.9157</td></tr><tr><td>eval/samples_per_second</td><td>72.078</td></tr><tr><td>eval/steps_per_second</td><td>18.566</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1893771667022592.0</td></tr><tr><td>train/train_loss</td><td>1.25948</td></tr><tr><td>train/train_runtime</td><td>99.5208</td></tr><tr><td>train/train_samples_per_second</td><td>18.338</td></tr><tr><td>train/train_steps_per_second</td><td>4.622</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-1-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/14l7q6pj\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/14l7q6pj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_171307-14l7q6pj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_171510-2k7slwlb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/2k7slwlb\" target=\"_blank\">model-64-1-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d6b85c828d4e0e89c015f23c7c33d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2b858ed9d44759bb91c7f2fa9d6292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56d9c20f769424e831d098e014e4c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-1-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-1-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a992ea42d74adfb8b576fd138ffe67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.14593</td></tr><tr><td>eval/rmse</td><td>1.4649</td></tr><tr><td>eval/runtime</td><td>0.7507</td></tr><tr><td>eval/samples_per_second</td><td>87.917</td></tr><tr><td>eval/steps_per_second</td><td>22.645</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896281602302720.0</td></tr><tr><td>train/train_loss</td><td>1.17945</td></tr><tr><td>train/train_runtime</td><td>107.7222</td></tr><tr><td>train/train_samples_per_second</td><td>16.942</td></tr><tr><td>train/train_steps_per_second</td><td>4.27</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-1-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/2k7slwlb\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/2k7slwlb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_171510-2k7slwlb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 4, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_171714-1nu467zv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1nu467zv\" target=\"_blank\">model-64-4-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1e720b02424a64a915470cc7223a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736545ef3c44421b846a485fcd1998d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c75fdf7d55d4c6e8f8bf31a37127231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-4-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-4-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0fbe0bc01c429196c266d773894358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.8044</td></tr><tr><td>eval/rmse</td><td>1.34328</td></tr><tr><td>eval/runtime</td><td>1.0254</td></tr><tr><td>eval/samples_per_second</td><td>64.365</td></tr><tr><td>eval/steps_per_second</td><td>16.579</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1892516699382528.0</td></tr><tr><td>train/train_loss</td><td>1.69143</td></tr><tr><td>train/train_runtime</td><td>90.1861</td></tr><tr><td>train/train_samples_per_second</td><td>20.236</td></tr><tr><td>train/train_steps_per_second</td><td>5.101</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-4-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/1nu467zv\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/1nu467zv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_171714-1nu467zv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 4, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_171901-rl4316ya</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/rl4316ya\" target=\"_blank\">model-64-4-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8a1b6823044c3f9837b03b81a5a3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28823018a562490e803b855c482b0d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d1610e67a84dd9bced263d4930c0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-4-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-4-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cf4c172a104beebd2c90ac16e34d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.03346</td></tr><tr><td>eval/rmse</td><td>1.42599</td></tr><tr><td>eval/runtime</td><td>0.6872</td></tr><tr><td>eval/samples_per_second</td><td>96.043</td></tr><tr><td>eval/steps_per_second</td><td>24.738</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1893771667022592.0</td></tr><tr><td>train/train_loss</td><td>1.2403</td></tr><tr><td>train/train_runtime</td><td>94.2277</td></tr><tr><td>train/train_samples_per_second</td><td>19.368</td></tr><tr><td>train/train_steps_per_second</td><td>4.882</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-4-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/rl4316ya\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/rl4316ya</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_171901-rl4316ya/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 4, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_172051-2z4je0wq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/2z4je0wq\" target=\"_blank\">model-64-4-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d2de42068e4061aa1b82be0d7e203b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9baa80ed1d8242dc9a584bd99176ca5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56701ba278334b7883a0d69c715b348b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-4-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-4-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a7bca9bb4a46b5951bd26143d87dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.28292</td></tr><tr><td>eval/rmse</td><td>1.51093</td></tr><tr><td>eval/runtime</td><td>0.8423</td></tr><tr><td>eval/samples_per_second</td><td>78.359</td></tr><tr><td>eval/steps_per_second</td><td>20.183</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896281602302720.0</td></tr><tr><td>train/train_loss</td><td>1.2236</td></tr><tr><td>train/train_runtime</td><td>104.4843</td></tr><tr><td>train/train_samples_per_second</td><td>17.467</td></tr><tr><td>train/train_steps_per_second</td><td>4.403</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-4-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/2z4je0wq\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/2z4je0wq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_172051-2z4je0wq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 8, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_172252-3myf33wj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3myf33wj\" target=\"_blank\">model-64-8-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fcfd1b803c4763baeb7c6afe2394bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c213366784f0423f81bf90c2e48b28a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51aad3f67c2647af8d04abc3ca0315eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-8-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-8-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df42d414bcd44d7b8bc27830891dd5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.08177</td></tr><tr><td>eval/rmse</td><td>1.44284</td></tr><tr><td>eval/runtime</td><td>0.776</td></tr><tr><td>eval/samples_per_second</td><td>85.054</td></tr><tr><td>eval/steps_per_second</td><td>21.908</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1892516699382528.0</td></tr><tr><td>train/train_loss</td><td>1.25671</td></tr><tr><td>train/train_runtime</td><td>93.4238</td></tr><tr><td>train/train_samples_per_second</td><td>19.535</td></tr><tr><td>train/train_steps_per_second</td><td>4.924</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-8-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3myf33wj\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3myf33wj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_172252-3myf33wj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 8, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_172443-8rlgkqpi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/8rlgkqpi\" target=\"_blank\">model-64-8-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a801639d3feb41f784d19f994ad94b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb81278fab674c6583edca32c1d204b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c60d3aa124742c0942ef11dedfc7ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-8-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-8-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23b7cc4ce0347ac88b4120f0df5af6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.98997</td></tr><tr><td>eval/rmse</td><td>1.41066</td></tr><tr><td>eval/runtime</td><td>0.6653</td></tr><tr><td>eval/samples_per_second</td><td>99.204</td></tr><tr><td>eval/steps_per_second</td><td>25.553</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1893771667022592.0</td></tr><tr><td>train/train_loss</td><td>1.18778</td></tr><tr><td>train/train_runtime</td><td>93.6204</td></tr><tr><td>train/train_samples_per_second</td><td>19.494</td></tr><tr><td>train/train_steps_per_second</td><td>4.913</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-8-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/8rlgkqpi\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/8rlgkqpi</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_172443-8rlgkqpi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 8, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_172632-2nlywb00</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/2nlywb00\" target=\"_blank\">model-64-8-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a35dc9f4ec45209424a8e3f2e5e4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070f8de6b09e4ec489f780e210b34a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47e0d7d950a4ba7bb44df82c403a39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-8-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-64-8-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5203de54f3ed467b8845bbc85dce9094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.38074</td></tr><tr><td>eval/rmse</td><td>1.54296</td></tr><tr><td>eval/runtime</td><td>0.7933</td></tr><tr><td>eval/samples_per_second</td><td>83.198</td></tr><tr><td>eval/steps_per_second</td><td>21.43</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896281602302720.0</td></tr><tr><td>train/train_loss</td><td>1.1957</td></tr><tr><td>train/train_runtime</td><td>109.5691</td></tr><tr><td>train/train_samples_per_second</td><td>16.656</td></tr><tr><td>train/train_steps_per_second</td><td>4.198</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-8-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/2nlywb00\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/2nlywb00</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_172632-2nlywb00/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 1, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_172839-3sebxlyn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3sebxlyn\" target=\"_blank\">model-128-1-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5040dd68ae1494b83cfc660d33fc494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1963300458364f00983a8e237440ac7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6335f562c7409ca35788e2a9fdc693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-1-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-1-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a5679e53df41149462bbaa00888dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.84091</td></tr><tr><td>eval/rmse</td><td>1.3568</td></tr><tr><td>eval/runtime</td><td>0.7861</td></tr><tr><td>eval/samples_per_second</td><td>83.958</td></tr><tr><td>eval/steps_per_second</td><td>21.626</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896423874437888.0</td></tr><tr><td>train/train_loss</td><td>1.29284</td></tr><tr><td>train/train_runtime</td><td>90.6883</td></tr><tr><td>train/train_samples_per_second</td><td>20.124</td></tr><tr><td>train/train_steps_per_second</td><td>5.072</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-1-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3sebxlyn\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3sebxlyn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_172839-3sebxlyn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 1, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_173027-16ourpc0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/16ourpc0\" target=\"_blank\">model-128-1-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245dde14938a4eb6973b12d9e884653c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e39cb9f54247e8abd260b511f941dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1603992aa8c4e6889aaf5e28121bdd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-1-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-1-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e003407714147a5a34b99c3e4f8f4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.90148</td></tr><tr><td>eval/rmse</td><td>1.37894</td></tr><tr><td>eval/runtime</td><td>0.7493</td></tr><tr><td>eval/samples_per_second</td><td>88.077</td></tr><tr><td>eval/steps_per_second</td><td>22.686</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1901380041057024.0</td></tr><tr><td>train/train_loss</td><td>1.20849</td></tr><tr><td>train/train_runtime</td><td>91.8956</td></tr><tr><td>train/train_samples_per_second</td><td>19.859</td></tr><tr><td>train/train_steps_per_second</td><td>5.006</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-1-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/16ourpc0\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/16ourpc0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_173027-16ourpc0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 1, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_173217-3e5bytp2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3e5bytp2\" target=\"_blank\">model-128-1-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f124652b6645cabf9e64d8a1e5c0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9f5d4ad7224d22ad4e3af90775a8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf28eb780d134eda9b5e8e97882ee783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:42, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-1-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-1-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af856e6e5bf4e4daeec3ef2e1e5cf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.70462</td></tr><tr><td>eval/rmse</td><td>1.30561</td></tr><tr><td>eval/runtime</td><td>0.7637</td></tr><tr><td>eval/samples_per_second</td><td>86.423</td></tr><tr><td>eval/steps_per_second</td><td>22.261</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911292374295296.0</td></tr><tr><td>train/train_loss</td><td>1.31704</td></tr><tr><td>train/train_runtime</td><td>103.1617</td></tr><tr><td>train/train_samples_per_second</td><td>17.691</td></tr><tr><td>train/train_steps_per_second</td><td>4.459</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-1-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3e5bytp2\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3e5bytp2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_173217-3e5bytp2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 4, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_173415-v7qjjbzi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/v7qjjbzi\" target=\"_blank\">model-128-4-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07285b789a464f61ba72a0fd12fa415e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb21dd4083041b7843069ab7c836cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5359f942404808b9dd29a4b55f4243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-4-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-4-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd70cf70dc94d9bae5dc35f77560932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.72467</td></tr><tr><td>eval/rmse</td><td>1.31327</td></tr><tr><td>eval/runtime</td><td>0.9859</td></tr><tr><td>eval/samples_per_second</td><td>66.945</td></tr><tr><td>eval/steps_per_second</td><td>17.243</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896423874437888.0</td></tr><tr><td>train/train_loss</td><td>1.18891</td></tr><tr><td>train/train_runtime</td><td>93.4885</td></tr><tr><td>train/train_samples_per_second</td><td>19.521</td></tr><tr><td>train/train_steps_per_second</td><td>4.92</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-4-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/v7qjjbzi\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/v7qjjbzi</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_173415-v7qjjbzi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 4, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_173605-2eh2a731</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/2eh2a731\" target=\"_blank\">model-128-4-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b600bfe74c534a968644ee8539142b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c3590ef9fc458e97441efcb4ad3e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e3f5fca3f84597b5e328a4fc7e60f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-4-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-4-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fb2def90504a80b32961ddb10844ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.10028</td></tr><tr><td>eval/rmse</td><td>1.44923</td></tr><tr><td>eval/runtime</td><td>0.7182</td></tr><tr><td>eval/samples_per_second</td><td>91.894</td></tr><tr><td>eval/steps_per_second</td><td>23.67</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1901380041057024.0</td></tr><tr><td>train/train_loss</td><td>1.3091</td></tr><tr><td>train/train_runtime</td><td>99.4282</td></tr><tr><td>train/train_samples_per_second</td><td>18.355</td></tr><tr><td>train/train_steps_per_second</td><td>4.626</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-4-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/2eh2a731\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/2eh2a731</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_173605-2eh2a731/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 4, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_173800-105hz2k4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/105hz2k4\" target=\"_blank\">model-128-4-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aee589177654dc88fb80d1d97dc4673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac258e05afab4defb8a69f5b43e9f1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ad9d8689054e03a107ed216bf5b8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-4-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-4-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513b52fe456c4eb8ae93a4562c0456d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.71839</td></tr><tr><td>eval/rmse</td><td>1.31087</td></tr><tr><td>eval/runtime</td><td>0.9468</td></tr><tr><td>eval/samples_per_second</td><td>69.711</td></tr><tr><td>eval/steps_per_second</td><td>17.956</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911292374295296.0</td></tr><tr><td>train/train_loss</td><td>1.31402</td></tr><tr><td>train/train_runtime</td><td>107.3494</td></tr><tr><td>train/train_samples_per_second</td><td>17.001</td></tr><tr><td>train/train_steps_per_second</td><td>4.285</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-4-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/105hz2k4\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/105hz2k4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_173800-105hz2k4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 8, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_174002-34b01ofx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/34b01ofx\" target=\"_blank\">model-128-8-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa600932e3174d78b923424328ee5207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da845654ac2041b2b9a8c3bab422fd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d716d13ee2744278707e545bdaa0c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-8-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-8-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07295db0b08d4343978ca4535bf14082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.81443</td></tr><tr><td>eval/rmse</td><td>1.34701</td></tr><tr><td>eval/runtime</td><td>0.7785</td></tr><tr><td>eval/samples_per_second</td><td>84.778</td></tr><tr><td>eval/steps_per_second</td><td>21.837</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896423874437888.0</td></tr><tr><td>train/train_loss</td><td>1.27628</td></tr><tr><td>train/train_runtime</td><td>91.054</td></tr><tr><td>train/train_samples_per_second</td><td>20.043</td></tr><tr><td>train/train_steps_per_second</td><td>5.052</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-8-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/34b01ofx\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/34b01ofx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_174002-34b01ofx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 8, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_174156-2155ffg9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/2155ffg9\" target=\"_blank\">model-128-8-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1106534a4274a48b4b367685eaaf776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45658c3ba0664764a15e2e6a77c006e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87eeafe5c8be4ad3a042e55eab5f575c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-8-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-8-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890ed806c8684a10b98bf8b8cc685c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.97739</td></tr><tr><td>eval/rmse</td><td>1.4062</td></tr><tr><td>eval/runtime</td><td>0.7826</td></tr><tr><td>eval/samples_per_second</td><td>84.34</td></tr><tr><td>eval/steps_per_second</td><td>21.724</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1901380041057024.0</td></tr><tr><td>train/train_loss</td><td>1.20381</td></tr><tr><td>train/train_runtime</td><td>98.2795</td></tr><tr><td>train/train_samples_per_second</td><td>18.569</td></tr><tr><td>train/train_steps_per_second</td><td>4.681</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-8-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/2155ffg9\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/2155ffg9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_174156-2155ffg9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 128, 'nhead': 8, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_174348-2zhjno4f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/2zhjno4f\" target=\"_blank\">model-128-8-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a8cce371b44bf2aa54feb73c37fee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23a82d206404d26b10c22b8d67f37dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9346d5360a7e4b24ba22e3b5c8c1073e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-8-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-128-8-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542def400eb8461a996e83917d416c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.17915</td></tr><tr><td>eval/rmse</td><td>1.47619</td></tr><tr><td>eval/runtime</td><td>0.8526</td></tr><tr><td>eval/samples_per_second</td><td>77.412</td></tr><tr><td>eval/steps_per_second</td><td>19.94</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911292374295296.0</td></tr><tr><td>train/train_loss</td><td>1.37241</td></tr><tr><td>train/train_runtime</td><td>109.3253</td></tr><tr><td>train/train_samples_per_second</td><td>16.693</td></tr><tr><td>train/train_steps_per_second</td><td>4.208</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-8-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/2zhjno4f\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/2zhjno4f</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_174348-2zhjno4f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 1, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_174553-2xnhzq13</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/2xnhzq13\" target=\"_blank\">model-256-1-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73e27a6b2e94b05980fd6157aa23c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbee331e98a1493592577d43c682fa14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69df1f04a1c47e3b5eefe3ee8c545a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-1-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-1-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0dfb08ad974b1fbd3b547226990cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.07724</td></tr><tr><td>eval/rmse</td><td>1.44126</td></tr><tr><td>eval/runtime</td><td>0.6231</td></tr><tr><td>eval/samples_per_second</td><td>105.919</td></tr><tr><td>eval/steps_per_second</td><td>27.282</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911984623788800.0</td></tr><tr><td>train/train_loss</td><td>1.39538</td></tr><tr><td>train/train_runtime</td><td>100.6572</td></tr><tr><td>train/train_samples_per_second</td><td>18.131</td></tr><tr><td>train/train_steps_per_second</td><td>4.57</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-1-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/2xnhzq13\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/2xnhzq13</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_174553-2xnhzq13/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 1, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_174750-3rxqsfk2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3rxqsfk2\" target=\"_blank\">model-256-1-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e41ccad96a444619f8d50a8af5a8c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c65a88fc224c1abb4d5a2c4a520f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576916d5593d4fb9972323987aeea8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-1-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-1-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d94a5d5f9c54e6995b473b08dcb6cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.27798</td></tr><tr><td>eval/rmse</td><td>1.5093</td></tr><tr><td>eval/runtime</td><td>0.7955</td></tr><tr><td>eval/samples_per_second</td><td>82.964</td></tr><tr><td>eval/steps_per_second</td><td>21.37</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1931681882383104.0</td></tr><tr><td>train/train_loss</td><td>1.27588</td></tr><tr><td>train/train_runtime</td><td>104.0553</td></tr><tr><td>train/train_samples_per_second</td><td>17.539</td></tr><tr><td>train/train_steps_per_second</td><td>4.421</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-1-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3rxqsfk2\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3rxqsfk2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_174750-3rxqsfk2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 1, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_174950-1b7fa5em</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1b7fa5em\" target=\"_blank\">model-256-1-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631f94496fd64219b624ff6f746b64fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada5076793564ac6bbe0664560b5949d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28516c496b1d4020bc741799c7639b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-1-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-1-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6c68035d6145b0897c9e77d977c518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.92298</td></tr><tr><td>eval/rmse</td><td>1.38672</td></tr><tr><td>eval/runtime</td><td>0.7072</td></tr><tr><td>eval/samples_per_second</td><td>93.33</td></tr><tr><td>eval/steps_per_second</td><td>24.04</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1971076399571712.0</td></tr><tr><td>train/train_loss</td><td>1.26653</td></tr><tr><td>train/train_runtime</td><td>110.1772</td></tr><tr><td>train/train_samples_per_second</td><td>16.564</td></tr><tr><td>train/train_steps_per_second</td><td>4.175</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-1-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/1b7fa5em\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/1b7fa5em</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_174950-1b7fa5em/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 4, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_175155-vhc80g2j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/vhc80g2j\" target=\"_blank\">model-256-4-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bed7f473a6344f78c39ff8ac0e2cf67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73663723298a4eb4820107ea6586a546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b1024d55ed4e9387f5c81734612f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-4-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-4-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e223bf104348909cd181893d9ea557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.21442</td></tr><tr><td>eval/rmse</td><td>1.48809</td></tr><tr><td>eval/runtime</td><td>0.6735</td></tr><tr><td>eval/samples_per_second</td><td>97.991</td></tr><tr><td>eval/steps_per_second</td><td>25.24</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911984623788800.0</td></tr><tr><td>train/train_loss</td><td>1.29505</td></tr><tr><td>train/train_runtime</td><td>96.5044</td></tr><tr><td>train/train_samples_per_second</td><td>18.911</td></tr><tr><td>train/train_steps_per_second</td><td>4.767</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-4-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/vhc80g2j\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/vhc80g2j</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_175155-vhc80g2j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 4, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_175347-ddm2df70</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/ddm2df70\" target=\"_blank\">model-256-4-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6d304e7ca24581a412361d538b39eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21c3be2fba04eec827da8492571bc9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cb09b1ad2547fd919b5787187ce347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-4-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-4-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8d8deca353448fb91c18eb1416c5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.21184</td></tr><tr><td>eval/rmse</td><td>1.48723</td></tr><tr><td>eval/runtime</td><td>0.6801</td></tr><tr><td>eval/samples_per_second</td><td>97.047</td></tr><tr><td>eval/steps_per_second</td><td>24.997</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1931681882383104.0</td></tr><tr><td>train/train_loss</td><td>1.28182</td></tr><tr><td>train/train_runtime</td><td>96.4623</td></tr><tr><td>train/train_samples_per_second</td><td>18.919</td></tr><tr><td>train/train_steps_per_second</td><td>4.769</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-4-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/ddm2df70\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/ddm2df70</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_175347-ddm2df70/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 4, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_175539-10so3g9t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/10so3g9t\" target=\"_blank\">model-256-4-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b4cb2c6c944fdcb70241f3e3ccb945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fa8516ebe94213a4cc55faecb1f515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20c4e852c3c4116a67a9c9af3ed85fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-4-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-4-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f412134c41534d618edcfbf440b0efe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.16818</td></tr><tr><td>eval/rmse</td><td>1.47247</td></tr><tr><td>eval/runtime</td><td>0.8052</td></tr><tr><td>eval/samples_per_second</td><td>81.969</td></tr><tr><td>eval/steps_per_second</td><td>21.113</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1971076399571712.0</td></tr><tr><td>train/train_loss</td><td>1.28367</td></tr><tr><td>train/train_runtime</td><td>104.9927</td></tr><tr><td>train/train_samples_per_second</td><td>17.382</td></tr><tr><td>train/train_steps_per_second</td><td>4.381</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-4-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/10so3g9t\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/10so3g9t</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_175539-10so3g9t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 8, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_175740-1y06v26k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1y06v26k\" target=\"_blank\">model-256-8-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefb3c00af324a369c7b5062355eed9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdac4a7b033647948f305339b31bd8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b32817def824bf89ec18dde99ad7829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-8-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-8-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5caddc28d4441a90ef538ca0049400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.86923</td></tr><tr><td>eval/rmse</td><td>1.3672</td></tr><tr><td>eval/runtime</td><td>0.5987</td></tr><tr><td>eval/samples_per_second</td><td>110.231</td></tr><tr><td>eval/steps_per_second</td><td>28.393</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911984623788800.0</td></tr><tr><td>train/train_loss</td><td>1.43483</td></tr><tr><td>train/train_runtime</td><td>88.5905</td></tr><tr><td>train/train_samples_per_second</td><td>20.6</td></tr><tr><td>train/train_steps_per_second</td><td>5.192</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-8-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/1y06v26k\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/1y06v26k</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_175740-1y06v26k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 8, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_175923-jkhmgm13</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/jkhmgm13\" target=\"_blank\">model-256-8-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e625c647a024ede9c0f4cd64af0ef88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47150f23df514275a93bf612eedf55b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c412fe12a84172b5612912e5dedd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-8-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-8-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132c95b42cfe40ce9044d9fa55fb2a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.30667</td></tr><tr><td>eval/rmse</td><td>1.51877</td></tr><tr><td>eval/runtime</td><td>0.6401</td></tr><tr><td>eval/samples_per_second</td><td>103.115</td></tr><tr><td>eval/steps_per_second</td><td>26.56</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1931681882383104.0</td></tr><tr><td>train/train_loss</td><td>1.27611</td></tr><tr><td>train/train_runtime</td><td>98.1581</td></tr><tr><td>train/train_samples_per_second</td><td>18.592</td></tr><tr><td>train/train_steps_per_second</td><td>4.686</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-8-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/jkhmgm13\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/jkhmgm13</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_175923-jkhmgm13/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 8, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_180117-2iorherw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/2iorherw\" target=\"_blank\">model-256-8-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737e37b8af2d4537a71fd034011661bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15558718bb8243f0842d26de12be430f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f13140aa7954a0a8437987535c71f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, topic, input_texts. If dlg_act_label, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: dlg_act_label, __index_level_0__, topic, input_texts. If dlg_act_label, __index_level_0__, topic, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-8-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/dlg-act-fine-tuning//model-256-8-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab57a49d84b4df58ed78161443fed55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.19756</td></tr><tr><td>eval/rmse</td><td>1.48242</td></tr><tr><td>eval/runtime</td><td>0.7225</td></tr><tr><td>eval/samples_per_second</td><td>91.349</td></tr><tr><td>eval/steps_per_second</td><td>23.529</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1971076399571712.0</td></tr><tr><td>train/train_loss</td><td>1.24954</td></tr><tr><td>train/train_runtime</td><td>105.6096</td></tr><tr><td>train/train_samples_per_second</td><td>17.281</td></tr><tr><td>train/train_steps_per_second</td><td>4.356</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-8-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/2iorherw\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/2iorherw</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_180117-2iorherw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [3, 6, 12]:\n",
    "            \n",
    "            experiments.train_and_evaluate_single_fold('hatformer', \n",
    "                   '../data/fine-tuning/dlg-act-fine-tuning/', \n",
    "                   eli5_training_df[['topic', 'input_texts', 'dlg_act_label', 'labels']], 'model-{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), \n",
    "                   extra_encoder_configs=[{'num_tokens':11, 'flow_model_hidden_size': flow_model_hidden_size, 'nhead': nhead, 'nlayers':nlayers}],\n",
    "                   lr=2e-5, batch_size=4,\n",
    "                   model_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Setting: 128-1-12 : 1.3056122064590454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64-1-3 : 1.4179565906524658\n",
      "64-1-6 : 1.4014121294021606\n",
      "64-1-12 : 1.4648977518081665\n",
      "64-4-3 : 1.3432799577713013\n",
      "64-4-6 : 1.4259941577911377\n",
      "64-4-12 : 1.510932445526123\n",
      "64-8-3 : 1.4428355693817139\n",
      "64-8-6 : 1.4106625318527222\n",
      "64-8-12 : 1.5429649353027344\n",
      "128-1-3 : 1.3568007946014404\n",
      "128-1-6 : 1.3789418935775757\n",
      "128-1-12 : 1.3056122064590454\n",
      "128-4-3 : 1.313267469406128\n",
      "128-4-6 : 1.4492336511611938\n",
      "128-4-12 : 1.3108749389648438\n",
      "128-8-3 : 1.3470078706741333\n",
      "128-8-6 : 1.406197190284729\n",
      "128-8-12 : 1.4761947393417358\n",
      "256-1-3 : 1.4412637948989868\n",
      "256-1-6 : 1.5092982053756714\n",
      "256-1-12 : 1.3867151737213135\n",
      "256-4-3 : 1.4880921840667725\n",
      "256-4-6 : 1.4872262477874756\n",
      "256-4-12 : 1.4724746942520142\n",
      "256-8-3 : 1.3671960830688477\n",
      "256-8-6 : 1.518773078918457\n",
      "256-8-12 : 1.482418179512024\n"
     ]
    }
   ],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [3, 6, 12]:\n",
    "            res = json.load(open('../data/fine-tuning/dlg-act-fine-tuning/model-{}-{}-{}/0-fold/eval_results.json'.format(flow_model_hidden_size, nhead, nlayers, flow_model_hidden_size, nhead, nlayers)))\n",
    "            print('{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), ':' , res['eval_rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Longformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: allenai/longformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/859f4633944e1b7e7fa301e72161388cd5903e36385d0ef2917256506bff64c3.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/af6fcabe2bf8cab6f77b20d94ba46a3dbf441ca0549e1f3c852c437b612f5224.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
      "Model config LongformerConfig {\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"onnx_export\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/allenai/longformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/a7a586602e625bd012d75abdfcc615f5bb1fe133273845f7381332c634273bd9.dc3a4f03d4ab11f972b126d0e6b67f43e5d9003b3aec54f8e549cc7e2d42398d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'classifier.dense.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'classifier.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'classifier.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230524_073859-5ks8f0w3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/5ks8f0w3\" target=\"_blank\">model-64-1-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input_clm=input_texts\n",
      "adding dlg_act_label to the flows\n",
      "Using input_clm=input_texts\n",
      "adding dlg_act_label to the flows\n",
      "Using input_clm=input_texts\n",
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 08:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/longformer/dlg-act-fine-tuning//model-64-1-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/longformer/dlg-act-fine-tuning//model-64-1-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea4f8901be540eebf6cdd1c046d31b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.27972</td></tr><tr><td>eval/rmse</td><td>1.13125</td></tr><tr><td>eval/runtime</td><td>4.4411</td></tr><tr><td>eval/samples_per_second</td><td>14.861</td></tr><tr><td>eval/steps_per_second</td><td>3.828</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>4829649022771200.0</td></tr><tr><td>train/train_loss</td><td>1.46197</td></tr><tr><td>train/train_runtime</td><td>515.7284</td></tr><tr><td>train/train_samples_per_second</td><td>3.539</td></tr><tr><td>train/train_steps_per_second</td><td>0.892</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-1-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/5ks8f0w3\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/5ks8f0w3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230524_073859-5ks8f0w3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: allenai/longformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/859f4633944e1b7e7fa301e72161388cd5903e36385d0ef2917256506bff64c3.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/af6fcabe2bf8cab6f77b20d94ba46a3dbf441ca0549e1f3c852c437b612f5224.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
      "Model config LongformerConfig {\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"onnx_export\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/allenai/longformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/a7a586602e625bd012d75abdfcc615f5bb1fe133273845f7381332c634273bd9.dc3a4f03d4ab11f972b126d0e6b67f43e5d9003b3aec54f8e549cc7e2d42398d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'classifier.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'classifier.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'classifier.dense.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'classifier.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230524_074757-k03w7zah</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/k03w7zah\" target=\"_blank\">model-64-1-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input_clm=input_texts\n",
      "adding dlg_act_label to the flows\n",
      "Using input_clm=input_texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding dlg_act_label to the flows\n",
      "Using input_clm=input_texts\n",
      "adding dlg_act_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 08:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/longformer/dlg-act-fine-tuning//model-64-1-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/longformer/dlg-act-fine-tuning//model-64-1-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002b685e264f45f283482f39a4694e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.29272</td></tr><tr><td>eval/rmse</td><td>1.13698</td></tr><tr><td>eval/runtime</td><td>4.5245</td></tr><tr><td>eval/samples_per_second</td><td>14.587</td></tr><tr><td>eval/steps_per_second</td><td>3.757</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>4833041926348800.0</td></tr><tr><td>train/train_loss</td><td>1.48706</td></tr><tr><td>train/train_runtime</td><td>518.5561</td></tr><tr><td>train/train_samples_per_second</td><td>3.519</td></tr><tr><td>train/train_steps_per_second</td><td>0.887</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-1-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/k03w7zah\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/k03w7zah</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230524_074757-k03w7zah/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: allenai/longformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/859f4633944e1b7e7fa301e72161388cd5903e36385d0ef2917256506bff64c3.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/af6fcabe2bf8cab6f77b20d94ba46a3dbf441ca0549e1f3c852c437b612f5224.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/allenai/longformer-base-4096/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"allenai/longformer-base-4096\",\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
      "Model config LongformerConfig {\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"onnx_export\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/allenai/longformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/a7a586602e625bd012d75abdfcc615f5bb1fe133273845f7381332c634273bd9.dc3a4f03d4ab11f972b126d0e6b67f43e5d9003b3aec54f8e549cc7e2d42398d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 11, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'classifier.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'classifier.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'classifier.dense.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'classifier.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230524_075654-1s4cb434</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1s4cb434\" target=\"_blank\">model-64-1-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input_clm=input_texts\n",
      "adding dlg_act_label to the flows\n",
      "Using input_clm=input_texts\n",
      "adding dlg_act_label to the flows\n",
      "Using input_clm=input_texts\n",
      "adding dlg_act_label to the flows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='310' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/460 05:58 < 02:54, 0.86 it/s, Epoch 3.36/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [3, 6, 12]:\n",
    "            experiments.train_and_evaluate_single_fold('longformer', \n",
    "                   '../data/fine-tuning/longformer/dlg-act-fine-tuning/', \n",
    "                   eli5_training_df[['topic', 'input_texts', 'dlg_act_label', 'labels']], 'model-{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), \n",
    "                   extra_encoder_configs=[{'num_tokens':11, 'flow_model_hidden_size': flow_model_hidden_size, 'nhead': nhead, 'nlayers':nlayers}],\n",
    "                   model_path=None, input_clm=\"input_texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64-1-3 : 1.1312487125396729\n",
      "64-1-6 : 1.1369800567626953\n",
      "64-1-12 : 1.185689091682434\n",
      "64-4-3 : 1.1839481592178345\n",
      "64-4-6 : 1.002856731414795\n",
      "64-4-12 : 1.2060630321502686\n",
      "64-8-3 : 1.2419322729110718\n",
      "64-8-6 : 1.1740511655807495\n",
      "64-8-12 : 1.0782896280288696\n",
      "128-1-3 : 1.157151460647583\n",
      "128-1-6 : 1.0975210666656494\n",
      "128-1-12 : 1.1209330558776855\n",
      "128-4-3 : 1.2515368461608887\n",
      "128-4-6 : 1.0999186038970947\n",
      "128-4-12 : 1.0980405807495117\n",
      "128-8-3 : 1.2256982326507568\n",
      "128-8-6 : 1.1302516460418701\n",
      "128-8-12 : 1.0665701627731323\n",
      "256-1-3 : 1.2297945022583008\n",
      "256-1-6 : 1.2704856395721436\n",
      "256-1-12 : 1.1250805854797363\n",
      "256-4-3 : 1.2377759218215942\n",
      "256-4-6 : 1.2722415924072266\n",
      "256-4-12 : 1.1695489883422852\n",
      "256-8-3 : 1.209364414215088\n",
      "256-8-6 : 1.2010687589645386\n",
      "256-8-12 : 1.2553132772445679\n"
     ]
    }
   ],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [3, 6, 12]:\n",
    "            res = json.load(open('../data/fine-tuning/longformer/dlg-act-fine-tuning/model-{}-{}-{}/0-fold/eval_results.json'.format(flow_model_hidden_size, nhead, nlayers, flow_model_hidden_size, nhead, nlayers)))\n",
    "            print('{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), ':' , res['eval_rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning for topic-rel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_180318-1cb8zdp1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1cb8zdp1\" target=\"_blank\">model-64-1-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126de47106d94820aa415f1faf5388dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da9ebd492844696ac20cc9f5066f77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdf5c1457b9408c960274b6652b5172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-1-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-1-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50c722cbe9c442ca3aa7caa21b7acd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.21644</td></tr><tr><td>eval/rmse</td><td>1.48877</td></tr><tr><td>eval/runtime</td><td>0.7133</td></tr><tr><td>eval/samples_per_second</td><td>92.523</td></tr><tr><td>eval/steps_per_second</td><td>23.832</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1892516699382528.0</td></tr><tr><td>train/train_loss</td><td>1.27934</td></tr><tr><td>train/train_runtime</td><td>87.1607</td></tr><tr><td>train/train_samples_per_second</td><td>20.938</td></tr><tr><td>train/train_steps_per_second</td><td>5.278</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-1-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/1cb8zdp1\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/1cb8zdp1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_180318-1cb8zdp1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_180500-2urgmkgv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/2urgmkgv\" target=\"_blank\">model-64-1-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28916c39a7454242a2c0144cc238b23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839fe694221a4e44bd8844eef8a39d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c02fa770e5f4e1986209a4bb56f1eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-1-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-1-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3492231ec6e9491bb71b5a11f067dfe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.98335</td></tr><tr><td>eval/rmse</td><td>1.40832</td></tr><tr><td>eval/runtime</td><td>0.6603</td></tr><tr><td>eval/samples_per_second</td><td>99.953</td></tr><tr><td>eval/steps_per_second</td><td>25.745</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1893771667022592.0</td></tr><tr><td>train/train_loss</td><td>1.2088</td></tr><tr><td>train/train_runtime</td><td>91.8088</td></tr><tr><td>train/train_samples_per_second</td><td>19.878</td></tr><tr><td>train/train_steps_per_second</td><td>5.01</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-1-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/2urgmkgv\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/2urgmkgv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_180500-2urgmkgv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 64, 'nhead': 1, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_180646-23qm62vh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/23qm62vh\" target=\"_blank\">model-64-1-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9d413d9f5d4df1b5f8be846e8201d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe39533c83f4d1b9120752e8f29ca3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe80f697c0d4f37bf2b35852b5b7b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-1-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-1-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d538b02d7ec4966800e3119dda36f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.04283</td></tr><tr><td>eval/rmse</td><td>1.42928</td></tr><tr><td>eval/runtime</td><td>0.6748</td></tr><tr><td>eval/samples_per_second</td><td>97.806</td></tr><tr><td>eval/steps_per_second</td><td>25.192</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896281602302720.0</td></tr><tr><td>train/train_loss</td><td>1.35024</td></tr><tr><td>train/train_runtime</td><td>103.8074</td></tr><tr><td>train/train_samples_per_second</td><td>17.581</td></tr><tr><td>train/train_steps_per_second</td><td>4.431</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-1-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/23qm62vh\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/23qm62vh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_180646-23qm62vh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 64, 'nhead': 4, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_180844-14r3x5np</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/14r3x5np\" target=\"_blank\">model-64-4-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd940e016f134c52bf1a3923ed866aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308488c0b9a84dcbb8ae478041d6c2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236e6546db7b4eeb88bc0c3d56f02699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-4-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-4-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1ec622e3534f6cb495437425942981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.74992</td></tr><tr><td>eval/rmse</td><td>1.32285</td></tr><tr><td>eval/runtime</td><td>0.6612</td></tr><tr><td>eval/samples_per_second</td><td>99.821</td></tr><tr><td>eval/steps_per_second</td><td>25.711</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1892516699382528.0</td></tr><tr><td>train/train_loss</td><td>1.52247</td></tr><tr><td>train/train_runtime</td><td>90.3677</td></tr><tr><td>train/train_samples_per_second</td><td>20.195</td></tr><tr><td>train/train_steps_per_second</td><td>5.09</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-4-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/14r3x5np\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/14r3x5np</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_180844-14r3x5np/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 64, 'nhead': 4, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_181029-2z2s3wdt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/2z2s3wdt\" target=\"_blank\">model-64-4-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0efb19b376143c0a21020cd4d459095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddfe986121c490684f847476f903bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fa9200f47446bc86ead3bac3344e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-4-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-4-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d820c6e246e0411fa8eb6bfcf3e887fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.1456</td></tr><tr><td>eval/rmse</td><td>1.46479</td></tr><tr><td>eval/runtime</td><td>0.6901</td></tr><tr><td>eval/samples_per_second</td><td>95.644</td></tr><tr><td>eval/steps_per_second</td><td>24.636</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1893771667022592.0</td></tr><tr><td>train/train_loss</td><td>1.19079</td></tr><tr><td>train/train_runtime</td><td>93.103</td></tr><tr><td>train/train_samples_per_second</td><td>19.602</td></tr><tr><td>train/train_steps_per_second</td><td>4.941</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-4-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/2z2s3wdt\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/2z2s3wdt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_181029-2z2s3wdt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 64, 'nhead': 4, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_181218-302nl247</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/302nl247\" target=\"_blank\">model-64-4-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8947a978ef6e4a3a942442394ff1d496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02004baa2a9a4d9fadc02de77fc3ffbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e928c220430845c1863eb50e4c1dcd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-4-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-4-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c58662d5e749328d6ee2203df619c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.99392</td></tr><tr><td>eval/rmse</td><td>1.41206</td></tr><tr><td>eval/runtime</td><td>0.713</td></tr><tr><td>eval/samples_per_second</td><td>92.566</td></tr><tr><td>eval/steps_per_second</td><td>23.843</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896281602302720.0</td></tr><tr><td>train/train_loss</td><td>1.3539</td></tr><tr><td>train/train_runtime</td><td>111.0172</td></tr><tr><td>train/train_samples_per_second</td><td>16.439</td></tr><tr><td>train/train_steps_per_second</td><td>4.144</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-4-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/302nl247\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/302nl247</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_181218-302nl247/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 64, 'nhead': 8, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_181425-216swb7l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/216swb7l\" target=\"_blank\">model-64-8-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6792f1e3245d4c73a3492fb0e3d7fd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcd14b585664e64a3e874b67b24114d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400733b14a3146a4aaa88a618f070fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-8-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-8-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3b715f6196467ea69255e3aa8d18bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.85165</td></tr><tr><td>eval/rmse</td><td>1.36076</td></tr><tr><td>eval/runtime</td><td>0.7035</td></tr><tr><td>eval/samples_per_second</td><td>93.815</td></tr><tr><td>eval/steps_per_second</td><td>24.164</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1892516699382528.0</td></tr><tr><td>train/train_loss</td><td>1.66869</td></tr><tr><td>train/train_runtime</td><td>92.9526</td></tr><tr><td>train/train_samples_per_second</td><td>19.634</td></tr><tr><td>train/train_steps_per_second</td><td>4.949</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-8-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/216swb7l\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/216swb7l</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_181425-216swb7l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 64, 'nhead': 8, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_181615-3d4u0zvg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3d4u0zvg\" target=\"_blank\">model-64-8-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2bbbc96f294ef1899fa611f86b7f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afff82e86fad4d3c951703b568d19085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2148f64fc174346b058cb371fa5f5e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-8-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-8-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4eb7786e6a4ca3abec506c42d963f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.55421</td></tr><tr><td>eval/rmse</td><td>1.59819</td></tr><tr><td>eval/runtime</td><td>0.7617</td></tr><tr><td>eval/samples_per_second</td><td>86.653</td></tr><tr><td>eval/steps_per_second</td><td>22.32</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1893771667022592.0</td></tr><tr><td>train/train_loss</td><td>1.15203</td></tr><tr><td>train/train_runtime</td><td>101.3504</td></tr><tr><td>train/train_samples_per_second</td><td>18.007</td></tr><tr><td>train/train_steps_per_second</td><td>4.539</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-8-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3d4u0zvg\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3d4u0zvg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_181615-3d4u0zvg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 64, 'nhead': 8, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_181818-cnbj6rq4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/cnbj6rq4\" target=\"_blank\">model-64-8-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cdab0364334c50ba4cfedcc318ca99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b526ed6c86b94a9fadfae4c8e13e847f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c598b475fd44474f98a9560375fde067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-8-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-64-8-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814e69cd63fd4dc781a4303c8a425909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.17002</td></tr><tr><td>eval/rmse</td><td>1.4731</td></tr><tr><td>eval/runtime</td><td>0.7641</td></tr><tr><td>eval/samples_per_second</td><td>86.377</td></tr><tr><td>eval/steps_per_second</td><td>22.249</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896281602302720.0</td></tr><tr><td>train/train_loss</td><td>1.3389</td></tr><tr><td>train/train_runtime</td><td>105.1257</td></tr><tr><td>train/train_samples_per_second</td><td>17.36</td></tr><tr><td>train/train_steps_per_second</td><td>4.376</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-64-8-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/cnbj6rq4\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/cnbj6rq4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_181818-cnbj6rq4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 128, 'nhead': 1, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_182018-mcdfj4v9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/mcdfj4v9\" target=\"_blank\">model-128-1-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1a584cb020423280df80acc44ff7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5f2b0def354b7198978d9aa5d7e812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b333e5c48f994202b5776ac721bdb4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-1-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-1-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e604f37ddd7646ad90f33ee414627ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.13228</td></tr><tr><td>eval/rmse</td><td>1.46023</td></tr><tr><td>eval/runtime</td><td>0.5983</td></tr><tr><td>eval/samples_per_second</td><td>110.304</td></tr><tr><td>eval/steps_per_second</td><td>28.412</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896423874437888.0</td></tr><tr><td>train/train_loss</td><td>1.34783</td></tr><tr><td>train/train_runtime</td><td>88.126</td></tr><tr><td>train/train_samples_per_second</td><td>20.709</td></tr><tr><td>train/train_steps_per_second</td><td>5.22</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-1-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/mcdfj4v9\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/mcdfj4v9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_182018-mcdfj4v9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 128, 'nhead': 1, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_182202-9y2hsw5w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/9y2hsw5w\" target=\"_blank\">model-128-1-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfafbcebff34d96819dba29ea90c6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d22e67620144956b69e751619030f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb532d56de104323bcb138b2ec613700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-1-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-1-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1874c7ed87604cc487052715e8300d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.81428</td></tr><tr><td>eval/rmse</td><td>1.34695</td></tr><tr><td>eval/runtime</td><td>0.7343</td></tr><tr><td>eval/samples_per_second</td><td>89.884</td></tr><tr><td>eval/steps_per_second</td><td>23.152</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1901380041057024.0</td></tr><tr><td>train/train_loss</td><td>1.27819</td></tr><tr><td>train/train_runtime</td><td>90.0173</td></tr><tr><td>train/train_samples_per_second</td><td>20.274</td></tr><tr><td>train/train_steps_per_second</td><td>5.11</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-1-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/9y2hsw5w\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/9y2hsw5w</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_182202-9y2hsw5w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 128, 'nhead': 1, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_182345-3kzsqqql</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3kzsqqql\" target=\"_blank\">model-128-1-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3c952d2472409397411d6df274dbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b18333000b490e9199a7285751aab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567e6039130f405ead67d54087fdd0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-1-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-1-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c342f11e0dc4c3a9eb78eff1a1fc42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.72976</td></tr><tr><td>eval/rmse</td><td>1.3152</td></tr><tr><td>eval/runtime</td><td>0.7481</td></tr><tr><td>eval/samples_per_second</td><td>88.22</td></tr><tr><td>eval/steps_per_second</td><td>22.723</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911292374295296.0</td></tr><tr><td>train/train_loss</td><td>1.97104</td></tr><tr><td>train/train_runtime</td><td>101.2527</td></tr><tr><td>train/train_samples_per_second</td><td>18.024</td></tr><tr><td>train/train_steps_per_second</td><td>4.543</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-1-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3kzsqqql\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3kzsqqql</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_182345-3kzsqqql/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 128, 'nhead': 4, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_182541-1yawzcf1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1yawzcf1\" target=\"_blank\">model-128-4-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37f20ae40384de68b1caedf152ff013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8846ddc1af94554b3494a0c34ade62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ee00af34f1498395982224e78cd45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-4-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-4-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0a6489c2224095a2f52c813bf0c345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.67912</td></tr><tr><td>eval/rmse</td><td>1.29581</td></tr><tr><td>eval/runtime</td><td>0.6513</td></tr><tr><td>eval/samples_per_second</td><td>101.331</td></tr><tr><td>eval/steps_per_second</td><td>26.1</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896423874437888.0</td></tr><tr><td>train/train_loss</td><td>1.73327</td></tr><tr><td>train/train_runtime</td><td>86.4675</td></tr><tr><td>train/train_samples_per_second</td><td>21.106</td></tr><tr><td>train/train_steps_per_second</td><td>5.32</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-4-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/1yawzcf1\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/1yawzcf1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_182541-1yawzcf1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 128, 'nhead': 4, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_182722-b35bnzeu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/b35bnzeu\" target=\"_blank\">model-128-4-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5928e23c8bb34c97a58a9941a5a05346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff5930419c54edf8aa2414605365437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a9539fe09d43d8bd0cf54731f4e20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-4-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-4-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f65e1e777e4bdd8fd40e3553a2eeb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.83397</td></tr><tr><td>eval/rmse</td><td>1.35424</td></tr><tr><td>eval/runtime</td><td>0.6159</td></tr><tr><td>eval/samples_per_second</td><td>107.156</td></tr><tr><td>eval/steps_per_second</td><td>27.601</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1901380041057024.0</td></tr><tr><td>train/train_loss</td><td>1.26457</td></tr><tr><td>train/train_runtime</td><td>91.2475</td></tr><tr><td>train/train_samples_per_second</td><td>20.001</td></tr><tr><td>train/train_steps_per_second</td><td>5.041</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-4-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/b35bnzeu\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/b35bnzeu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_182722-b35bnzeu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 128, 'nhead': 4, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_182909-33q9df8b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/33q9df8b\" target=\"_blank\">model-128-4-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f71082ec2c472ea74be31be493218c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7e4ebdea5b46a1886a72e149956a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785f67d560584f3fb63d6a627fcccb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-4-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-4-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0699f4d9e449efaf4eef8a0dec9eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.89407</td></tr><tr><td>eval/rmse</td><td>1.37625</td></tr><tr><td>eval/runtime</td><td>0.7022</td></tr><tr><td>eval/samples_per_second</td><td>93.992</td></tr><tr><td>eval/steps_per_second</td><td>24.21</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911292374295296.0</td></tr><tr><td>train/train_loss</td><td>1.32575</td></tr><tr><td>train/train_runtime</td><td>103.9124</td></tr><tr><td>train/train_samples_per_second</td><td>17.563</td></tr><tr><td>train/train_steps_per_second</td><td>4.427</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-4-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/33q9df8b\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/33q9df8b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_182909-33q9df8b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 128, 'nhead': 8, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_183106-19mjtzyv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/19mjtzyv\" target=\"_blank\">model-128-8-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf664d18a3a4ff18e48fc2ece6fbcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8df77b946d41d78a8f9b2f137edd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd04193cbc34bb7b12477b69b5b9778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-8-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-8-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874d2993a42d4204a62758a6ab4cc018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.95911</td></tr><tr><td>eval/rmse</td><td>1.39968</td></tr><tr><td>eval/runtime</td><td>0.7152</td></tr><tr><td>eval/samples_per_second</td><td>92.286</td></tr><tr><td>eval/steps_per_second</td><td>23.771</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1896423874437888.0</td></tr><tr><td>train/train_loss</td><td>1.22429</td></tr><tr><td>train/train_runtime</td><td>99.6508</td></tr><tr><td>train/train_samples_per_second</td><td>18.314</td></tr><tr><td>train/train_steps_per_second</td><td>4.616</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-8-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/19mjtzyv\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/19mjtzyv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_183106-19mjtzyv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 128, 'nhead': 8, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_183302-36hb31zy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/36hb31zy\" target=\"_blank\">model-128-8-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6f6ebd5df7499e931e51e307d05a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386ec14d611c4caea988672f2da13283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976f2bba19d04da7b1e84f7a43cefb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-8-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-8-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d3dee48592422c865f11100910b57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.81088</td></tr><tr><td>eval/rmse</td><td>1.34569</td></tr><tr><td>eval/runtime</td><td>0.8844</td></tr><tr><td>eval/samples_per_second</td><td>74.631</td></tr><tr><td>eval/steps_per_second</td><td>19.223</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1901380041057024.0</td></tr><tr><td>train/train_loss</td><td>1.33105</td></tr><tr><td>train/train_runtime</td><td>104.994</td></tr><tr><td>train/train_samples_per_second</td><td>17.382</td></tr><tr><td>train/train_steps_per_second</td><td>4.381</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-8-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/36hb31zy\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/36hb31zy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_183302-36hb31zy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 128, 'nhead': 8, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_183508-22gs4b6a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/22gs4b6a\" target=\"_blank\">model-128-8-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4643f92fe14b27b560629f3f624886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2596f6f5ada24891b625baefaefc7e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfb783e390945f7b4ddc56d71308377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-8-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-128-8-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13239cc3fa64ad89e98f341b783deb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.72586</td></tr><tr><td>eval/rmse</td><td>1.31372</td></tr><tr><td>eval/runtime</td><td>0.9443</td></tr><tr><td>eval/samples_per_second</td><td>69.891</td></tr><tr><td>eval/steps_per_second</td><td>18.002</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911292374295296.0</td></tr><tr><td>train/train_loss</td><td>1.30722</td></tr><tr><td>train/train_runtime</td><td>115.0274</td></tr><tr><td>train/train_samples_per_second</td><td>15.866</td></tr><tr><td>train/train_steps_per_second</td><td>3.999</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-128-8-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/22gs4b6a\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/22gs4b6a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_183508-22gs4b6a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 256, 'nhead': 1, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_183723-3j0kfvys</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3j0kfvys\" target=\"_blank\">model-256-1-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11682f05494d4dfba85b0acc7141b6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ab584f95e54db2bdcc7c3511ad5ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34d3ccb53e14b408dd1173b7b794de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-1-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-1-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079d4e2fbf05428ca1b39dbeb52f94be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.99116</td></tr><tr><td>eval/rmse</td><td>1.41108</td></tr><tr><td>eval/runtime</td><td>1.3687</td></tr><tr><td>eval/samples_per_second</td><td>48.223</td></tr><tr><td>eval/steps_per_second</td><td>12.421</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911984623788800.0</td></tr><tr><td>train/train_loss</td><td>1.25553</td></tr><tr><td>train/train_runtime</td><td>100.2726</td></tr><tr><td>train/train_samples_per_second</td><td>18.2</td></tr><tr><td>train/train_steps_per_second</td><td>4.587</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-1-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3j0kfvys\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3j0kfvys</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_183723-3j0kfvys/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 256, 'nhead': 1, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_183922-3fo5zds0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3fo5zds0\" target=\"_blank\">model-256-1-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cadf8cd7d5499098072f56a3efb52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38a871f9b694913a287edc918f33948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd212062a9f4f639ef05d57ff3f4d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-1-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-1-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2812451901e4d87a686de99fcdbf92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.06229</td></tr><tr><td>eval/rmse</td><td>1.43607</td></tr><tr><td>eval/runtime</td><td>0.6878</td></tr><tr><td>eval/samples_per_second</td><td>95.958</td></tr><tr><td>eval/steps_per_second</td><td>24.716</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1931681882383104.0</td></tr><tr><td>train/train_loss</td><td>1.37824</td></tr><tr><td>train/train_runtime</td><td>101.1582</td></tr><tr><td>train/train_samples_per_second</td><td>18.041</td></tr><tr><td>train/train_steps_per_second</td><td>4.547</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-1-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3fo5zds0\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3fo5zds0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_183922-3fo5zds0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 256, 'nhead': 1, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_184150-340401pt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/340401pt\" target=\"_blank\">model-256-1-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a71269cdb14839ba3d223b77f72332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df88779c7824c13a6df3f72d6fdc391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e0da88fc204dc3a46c7d254ebcb81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-1-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-1-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251d8a6d1ff94202a5d43c322de76951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.1741</td></tr><tr><td>eval/rmse</td><td>1.47448</td></tr><tr><td>eval/runtime</td><td>0.7389</td></tr><tr><td>eval/samples_per_second</td><td>89.319</td></tr><tr><td>eval/steps_per_second</td><td>23.006</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1971076399571712.0</td></tr><tr><td>train/train_loss</td><td>1.27096</td></tr><tr><td>train/train_runtime</td><td>103.8063</td></tr><tr><td>train/train_samples_per_second</td><td>17.581</td></tr><tr><td>train/train_steps_per_second</td><td>4.431</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-1-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/340401pt\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/340401pt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_184150-340401pt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 256, 'nhead': 4, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_184350-ymjq2di7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/ymjq2di7\" target=\"_blank\">model-256-4-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffa6077739f46c3abcd068c7a0d46a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb87f692d53e402787b7a9230cbcc34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c122a2811ad402ea4ea550e64a57a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-4-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-4-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3a3d324dbc4840a2355287ec1af335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.09689</td></tr><tr><td>eval/rmse</td><td>1.44807</td></tr><tr><td>eval/runtime</td><td>0.6638</td></tr><tr><td>eval/samples_per_second</td><td>99.429</td></tr><tr><td>eval/steps_per_second</td><td>25.611</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911984623788800.0</td></tr><tr><td>train/train_loss</td><td>1.30195</td></tr><tr><td>train/train_runtime</td><td>92.2054</td></tr><tr><td>train/train_samples_per_second</td><td>19.793</td></tr><tr><td>train/train_steps_per_second</td><td>4.989</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-4-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/ymjq2di7\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/ymjq2di7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_184350-ymjq2di7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 256, 'nhead': 4, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_184539-1whunh13</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1whunh13\" target=\"_blank\">model-256-4-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16eb8990e0594fa0bac0f03e4ec0df98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493e19f505db4246beaaeb1d2f4a1a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fe755b45db4cfc925178ca32cb314b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-4-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-4-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8f12be04f245f4b7d066c371eabd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.89658</td></tr><tr><td>eval/rmse</td><td>1.37716</td></tr><tr><td>eval/runtime</td><td>0.7578</td></tr><tr><td>eval/samples_per_second</td><td>87.093</td></tr><tr><td>eval/steps_per_second</td><td>22.433</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1931681882383104.0</td></tr><tr><td>train/train_loss</td><td>1.42353</td></tr><tr><td>train/train_runtime</td><td>97.3522</td></tr><tr><td>train/train_samples_per_second</td><td>18.746</td></tr><tr><td>train/train_steps_per_second</td><td>4.725</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-4-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/1whunh13\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/1whunh13</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_184539-1whunh13/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 256, 'nhead': 4, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_184733-22bd3dcn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/22bd3dcn\" target=\"_blank\">model-256-4-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6664da62f68d48a3a0fa3695e6dbaa69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a016477f464d97a5d7929a28984259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834400a9240f4f8d92eb9c9cbc709ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-4-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-4-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3731e25082e847849de454214bc0e4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.03439</td></tr><tr><td>eval/rmse</td><td>1.42632</td></tr><tr><td>eval/runtime</td><td>1.0393</td></tr><tr><td>eval/samples_per_second</td><td>63.507</td></tr><tr><td>eval/steps_per_second</td><td>16.358</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1971076399571712.0</td></tr><tr><td>train/train_loss</td><td>1.28433</td></tr><tr><td>train/train_runtime</td><td>109.6075</td></tr><tr><td>train/train_samples_per_second</td><td>16.65</td></tr><tr><td>train/train_steps_per_second</td><td>4.197</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-4-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/22bd3dcn\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/22bd3dcn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_184733-22bd3dcn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 256, 'nhead': 8, 'nlayers': 3}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_184939-3c5h93tr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/3c5h93tr\" target=\"_blank\">model-256-8-3</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b309a328b8c4e50bb2177a068710eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5697008b4b4b432db1e1b3d2162cedaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df711cbac7dc47b9a2276b4a2ed83652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-8-3/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-8-3/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0415a0fcac49465eb0824de6d928cddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.29531</td></tr><tr><td>eval/rmse</td><td>1.51503</td></tr><tr><td>eval/runtime</td><td>0.8625</td></tr><tr><td>eval/samples_per_second</td><td>76.525</td></tr><tr><td>eval/steps_per_second</td><td>19.711</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1911984623788800.0</td></tr><tr><td>train/train_loss</td><td>1.33797</td></tr><tr><td>train/train_runtime</td><td>90.7585</td></tr><tr><td>train/train_samples_per_second</td><td>20.108</td></tr><tr><td>train/train_steps_per_second</td><td>5.068</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-8-3</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/3c5h93tr\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/3c5h93tr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_184939-3c5h93tr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 256, 'nhead': 8, 'nlayers': 6}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_185126-1ehqs1lf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1ehqs1lf\" target=\"_blank\">model-256-8-6</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a50d818246c4fa49d6efe2a3c64d6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76b9234cc51479b8e8f7e26cf79db7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccd5489efdd4218879fdd0c56671fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-8-6/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-8-6/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f90d806217466e84a2dcf6af2406a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.84266</td></tr><tr><td>eval/rmse</td><td>1.35745</td></tr><tr><td>eval/runtime</td><td>0.6318</td></tr><tr><td>eval/samples_per_second</td><td>104.472</td></tr><tr><td>eval/steps_per_second</td><td>26.909</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1931681882383104.0</td></tr><tr><td>train/train_loss</td><td>1.3353</td></tr><tr><td>train/train_runtime</td><td>93.6309</td></tr><tr><td>train/train_samples_per_second</td><td>19.491</td></tr><tr><td>train/train_steps_per_second</td><td>4.913</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-8-6</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/1ehqs1lf\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/1ehqs1lf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_185126-1ehqs1lf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: kiddothe2b/hierarchical-transformer-base-4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"finetuning_task\": \"document-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/vocab.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/b43ba3b1768a9480d7c985b0c22a3b10c15f0afd581f05347a1beb7a7f145eb1.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/merges.txt from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7177ee98ea6eb708be40ec3560f4807b7bf196fdf8f110dc4c249053bb47bef5.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/special_tokens_map.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/d87e91220b4f8fe445492c0ca87879d99261d9ace150b1cd71c913c14ae66872.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/tokenizer_config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/7e7e7119ad1bc49907efe0f7f053bc0d2faf4ac6ebce153e4f7576436bf97df1.829f8fd1fa96172393d7b8208d8ed93ee137f1042e291df240a0a8e7f1699142\n",
      "loading configuration file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/config.json from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/67fff7e4e0a33106e11c355770df4420b2385850d788d1b7b549eb8d64b4f3d1.ef5baf2314e8ee19e0e25bc7741c47229b1ac5c08ae795cbc2d27f5f2c19535c\n",
      "Model config HATConfig {\n",
      "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
      "  \"architectures\": [\n",
      "    \"HATForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
      "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
      "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
      "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
      "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
      "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
      "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
      "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
      "  },\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"encoder_layout\": {\n",
      "    \"0\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"1\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"10\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"11\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"document_encoder\": true,\n",
      "      \"sentence_encoder\": true\n",
      "    },\n",
      "    \"9\": {\n",
      "      \"document_encoder\": false,\n",
      "      \"sentence_encoder\": true\n",
      "    }\n",
      "  },\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_encoders_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"max_sentence_length\": 128,\n",
      "  \"max_sentence_size\": 128,\n",
      "  \"max_sentences\": 32,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"hierarchical-transformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"parameters\": 136350720,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/kiddothe2b/hierarchical-transformer-base-4096/resolve/main/pytorch_model.bin from cache at /mnt/ceph/storage/data-tmp/current/sile2804/.cache/huggingface/transformers/3e6b4d161afa24393cd7a5363c4581beaea6fcad5e0aa6e1efd3733999473b92.b9741b26fc86ac9ac410a55dc25f6ed6b2dff47efeefb6a7058e3e63117e545a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra encoders:  [{'num_tokens': 5, 'flow_model_hidden_size': 256, 'nhead': 8, 'nlayers': 12}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing MyHATForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyHATForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MyHATForSequenceClassification were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['extra_encoders.0.transformer_encoder.layers.10.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear1.weight', 'extra_poolers.0.dense.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.5.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.10.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.9.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.weight', 'pooler.dense.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.9.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.4.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.5.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.2.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.0.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.7.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.3.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear2.bias', 'extra_poolers.0.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.2.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.10.self_attn.in_proj_bias', 'pooler.dense.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.3.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.6.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.3.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.7.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.4.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.11.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.4.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.10.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.0.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.2.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.5.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.0.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.3.norm2.weight', 'classifier.bias', 'extra_encoders.0.transformer_encoder.layers.2.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.8.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.7.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.11.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.5.linear2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_bias', 'extra_encoders.0.pos_encoder.pe', 'extra_encoders.0.transformer_encoder.layers.11.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.3.norm1.weight', 'extra_encoders.0.embedding.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.7.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.7.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.4.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.8.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.9.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear1.weight', 'extra_encoders.0.transformer_encoder.layers.8.norm1.bias', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.6.linear1.bias', 'extra_encoders.0.transformer_encoder.layers.6.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.8.linear2.weight', 'classifier.weight', 'extra_encoders.0.transformer_encoder.layers.1.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.11.self_attn.in_proj_bias', 'extra_encoders.0.transformer_encoder.layers.7.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.1.self_attn.in_proj_weight', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.bias', 'extra_encoders.0.transformer_encoder.layers.11.norm2.bias', 'extra_encoders.0.transformer_encoder.layers.2.self_attn.out_proj.weight', 'extra_encoders.0.transformer_encoder.layers.0.linear2.bias', 'extra_encoders.0.transformer_encoder.layers.1.norm1.weight', 'extra_encoders.0.transformer_encoder.layers.10.norm2.weight', 'extra_encoders.0.transformer_encoder.layers.0.self_attn.in_proj_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.20"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sile2804/explanation_assessment/eli5-experiments/src-ipynb/wandb/run-20230523_185321-1akrr5pc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/milad-it/test-project/runs/1akrr5pc\" target=\"_blank\">model-256-8-12</a></strong> to <a href=\"https://wandb.ai/milad-it/test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67aa03e29e1847538f54cd7d5a3d5f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e417e2d83fa74344bd6cdc33d9dafb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015d92b3b388409cabf89b685bc65501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding topic_func_label to the flows\n",
      "Training 365, Valid 66, and Test 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: topic, topic_func_label, input_texts. If topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 365\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [460/460 01:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MyHATForSequenceClassification.forward` and have been ignored: __index_level_0__, topic, topic_func_label, input_texts. If __index_level_0__, topic, topic_func_label, input_texts are not expected by `MyHATForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-8-12/0-fold/config.json\n",
      "Model weights saved in ../data/fine-tuning/topic-func-fine-tuning//model-256-8-12/0-fold/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6689c0b6430c46188a90e91b5ecf0d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rmse</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.81027</td></tr><tr><td>eval/rmse</td><td>1.34546</td></tr><tr><td>eval/runtime</td><td>0.749</td></tr><tr><td>eval/samples_per_second</td><td>88.115</td></tr><tr><td>eval/steps_per_second</td><td>22.696</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>460</td></tr><tr><td>train/total_flos</td><td>1971076399571712.0</td></tr><tr><td>train/train_loss</td><td>1.36621</td></tr><tr><td>train/train_runtime</td><td>105.316</td></tr><tr><td>train/train_samples_per_second</td><td>17.329</td></tr><tr><td>train/train_steps_per_second</td><td>4.368</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">model-256-8-12</strong>: <a href=\"https://wandb.ai/milad-it/test-project/runs/1akrr5pc\" target=\"_blank\">https://wandb.ai/milad-it/test-project/runs/1akrr5pc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230523_185321-1akrr5pc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [3, 6, 12]:\n",
    "            \n",
    "            experiments.train_and_evaluate_single_fold('hatformer', \n",
    "                   '../data/fine-tuning/topic-func-fine-tuning/', \n",
    "                   eli5_training_df[['topic', 'input_texts', 'topic_func_label', 'labels']], 'model-{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), \n",
    "                   extra_encoder_configs=[{'num_tokens':5, 'flow_model_hidden_size': flow_model_hidden_size, 'nhead': nhead, 'nlayers':nlayers}],\n",
    "                   lr=2e-5, batch_size=4, model_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best settings: 128-4-3 : 1.295806884765625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64-1-3 : 1.4887714385986328\n",
      "64-1-6 : 1.4083162546157837\n",
      "64-1-12 : 1.4292759895324707\n",
      "64-4-3 : 1.3228458166122437\n",
      "64-4-6 : 1.464785099029541\n",
      "64-4-12 : 1.412062406539917\n",
      "64-8-3 : 1.3607553243637085\n",
      "64-8-6 : 1.5981898307800293\n",
      "64-8-12 : 1.4731000661849976\n",
      "128-1-3 : 1.4602335691452026\n",
      "128-1-6 : 1.3469520807266235\n",
      "128-1-12 : 1.3152031898498535\n",
      "128-4-3 : 1.295806884765625\n",
      "128-4-6 : 1.3542428016662598\n",
      "128-4-12 : 1.3762528896331787\n",
      "128-8-3 : 1.3996809720993042\n",
      "128-8-6 : 1.3456896543502808\n",
      "128-8-12 : 1.3137216567993164\n",
      "256-1-3 : 1.4110838174819946\n",
      "256-1-6 : 1.4360675811767578\n",
      "256-1-12 : 1.4744844436645508\n",
      "256-4-3 : 1.448065161705017\n",
      "256-4-6 : 1.3771631717681885\n",
      "256-4-12 : 1.426318883895874\n",
      "256-8-3 : 1.515027403831482\n",
      "256-8-6 : 1.3574457168579102\n",
      "256-8-12 : 1.3454622030258179\n"
     ]
    }
   ],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [3, 6, 12]:\n",
    "            res = json.load(open('../data/fine-tuning/topic-func-fine-tuning/model-{}-{}-{}/0-fold/eval_results.json'.format(flow_model_hidden_size, nhead, nlayers, flow_model_hidden_size, nhead, nlayers)))\n",
    "            print('{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), ':' , res['eval_rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Longformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [3, 6, 12]:\n",
    "            \n",
    "            experiments.train_and_evaluate_single_fold('longformer', \n",
    "                   '../data/fine-tuning/longformer/topic-func-fine-tuning/', \n",
    "                   eli5_training_df[['topic', 'input_texts', 'topic_func_label', 'labels']], 'model-{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), \n",
    "                   extra_encoder_configs=[{'num_tokens':5, 'flow_model_hidden_size': flow_model_hidden_size, 'nhead': nhead, 'nlayers':nlayers}],\n",
    "                   model_path=None, input_clm=\"input_texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64-1-3 : 1.15657639503479\n",
      "64-1-6 : 1.1741114854812622\n",
      "64-1-12 : 1.1194347143173218\n",
      "64-4-3 : 1.2248600721359253\n",
      "64-4-6 : 1.185341715812683\n",
      "64-4-12 : 1.1410428285598755\n",
      "64-8-3 : 1.0858769416809082\n",
      "64-8-6 : 1.2097516059875488\n",
      "64-8-12 : 1.1560089588165283\n",
      "128-1-3 : 1.0691078901290894\n",
      "128-1-6 : 1.0931344032287598\n",
      "128-1-12 : 1.2045587301254272\n",
      "128-4-3 : 1.0940732955932617\n",
      "128-4-6 : 1.159157156944275\n",
      "128-4-12 : 1.2112083435058594\n",
      "128-8-3 : 1.1551358699798584\n",
      "128-8-6 : 1.1457695960998535\n",
      "128-8-12 : 1.2548877000808716\n",
      "256-1-3 : 1.1511489152908325\n",
      "256-1-6 : 1.1237391233444214\n",
      "256-1-12 : 1.2820554971694946\n",
      "256-4-3 : 1.0994726419448853\n",
      "256-4-6 : 1.104769229888916\n",
      "256-4-12 : 1.2569202184677124\n",
      "256-8-3 : 1.156984567642212\n",
      "256-8-6 : 1.08202064037323\n",
      "256-8-12 : 1.1515084505081177\n"
     ]
    }
   ],
   "source": [
    "for flow_model_hidden_size in [64, 128, 256]:\n",
    "    for nhead in [1, 4, 8]:\n",
    "        for nlayers in [3, 6, 12]:\n",
    "            res = json.load(open('../data/fine-tuning/longformer/topic-func-fine-tuning/model-{}-{}-{}/0-fold/eval_results.json'.format(flow_model_hidden_size, nhead, nlayers, flow_model_hidden_size, nhead, nlayers)))\n",
    "            print('{}-{}-{}'.format(flow_model_hidden_size, nhead, nlayers), ':' , res['eval_rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Parameters found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_former_feats_clms_encoders = {\n",
    "    \"exp_act_label\": {'num_tokens':11, 'flow_model_hidden_size': 128, 'nhead':8, 'nlayers':12},\n",
    "    \"dlg_act_label\": {'num_tokens':11, 'flow_model_hidden_size': 128, 'nhead':1, 'nlayers':12},\n",
    "    \"topic_func_label\": {'num_tokens':5, 'flow_model_hidden_size': 128, 'nhead':4, 'nlayers':3}\n",
    "}\n",
    "\n",
    "long_former_feats_clms_encoders = {\n",
    "    \"exp_act_label\": {'num_tokens':11, 'flow_model_hidden_size': 128, 'nhead':4, 'nlayers':6},\n",
    "    \"dlg_act_label\": {'num_tokens':11, 'flow_model_hidden_size': 128, 'nhead':1, 'nlayers':12},\n",
    "    \"topic_func_label\": {'num_tokens':5, 'flow_model_hidden_size': 128, 'nhead':8, 'nlayers':12}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing kfolds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 163, and Test 41\n",
      "Training 553, and Test 99\n",
      "------\n",
      "Training 163, and Test 41\n",
      "Training 532, and Test 120\n",
      "------\n",
      "Training 163, and Test 41\n",
      "Training 485, and Test 167\n",
      "------\n",
      "Training 163, and Test 41\n",
      "Training 481, and Test 171\n",
      "------\n",
      "Training 164, and Test 40\n",
      "Training 557, and Test 95\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "#split the two corpora\n",
    "topics  = eli5_annotation_df.topic.unique()\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "#kfold  = StratifiedKFold(n_splits=n_folds)\n",
    "fold_idx = 0\n",
    "rmse_scores = []\n",
    "for fold in kfold.split(topics):\n",
    "    train_topics = topics[fold[0]]\n",
    "    test_topics = topics[fold[1]]\n",
    "    \n",
    "    test_df  = eli5_annotation_df[eli5_annotation_df.topic.isin(test_topics)]\n",
    "    train_df = eli5_annotation_df[eli5_annotation_df.topic.isin(train_topics)]\n",
    "    \n",
    "    print('Training {}, and Test {}'.format(len(train_topics), len(test_topics)))\n",
    "    print('Training {}, and Test {}'.format(len(train_df), len(test_df)))\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df.groupby('task_id').agg({\n",
    "    'exp_act_label_predictions': lambda x : list(x)\n",
    "}).reset_index()\n",
    "pred_df_dict = pd.Series(pred_df.exp_act_label_predictions.values, index=pred_df.task_id).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5_annotation_df['exp_act_label'] = eli5_annotation_df.task_id.apply(lambda x: pred_df_dict[x])\n",
    "eli5_annotation_df['exp_act_label'] =  eli5_annotation_df['exp_act_label'].apply(lambda row: [int(x[2:4]) for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Evalauting model ../data/quality_models/hat-models/hat-model-exp-act-encoder/ ------\n",
      "----- testing fold 0 ------\n",
      "Loading model: ../data/quality_models/hat-models/hat-model-exp-act-encoder//0-fold/\n",
      "Extra encoders:  dict_values([{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 1, 'nlayers': 3}])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function evaluate_model_on_fold.<locals>.<lambda> at 0x7fcd07154c10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e9378c978f4f10bc78e48a4dfdb416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "----- testing fold 1 ------\n",
      "Loading model: ../data/quality_models/hat-models/hat-model-exp-act-encoder//1-fold/\n",
      "Extra encoders:  dict_values([{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 1, 'nlayers': 3}])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf026035186b41749abbdc10cd745098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "----- testing fold 2 ------\n",
      "Loading model: ../data/quality_models/hat-models/hat-model-exp-act-encoder//2-fold/\n",
      "Extra encoders:  dict_values([{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 1, 'nlayers': 3}])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97a137c8bca4f0196a8dd6da632634f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "----- testing fold 3 ------\n",
      "Loading model: ../data/quality_models/hat-models/hat-model-exp-act-encoder//3-fold/\n",
      "Extra encoders:  dict_values([{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 1, 'nlayers': 3}])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d88c002c29478a8ae4fa2383e60e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "----- testing fold 4 ------\n",
      "Loading model: ../data/quality_models/hat-models/hat-model-exp-act-encoder//4-fold/\n",
      "Extra encoders:  dict_values([{'num_tokens': 11, 'flow_model_hidden_size': 256, 'nhead': 1, 'nlayers': 3}])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a160b5e1ab54dfc925230c6c4018d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding exp_act_label to the flows\n",
      "[1.4288688506008225, 1.3722334508959408, 1.489365735140129, 1.389072371600225, 1.2602452143061655]\n",
      "hat-enc-exp-act (predictions):  1.3879571245086566 1.0832064471683118\n"
     ]
    }
   ],
   "source": [
    "hat_enc_exp_act_mse, hat_enc_exp_act_mae, _, _ = evaluate_model(eli5_annotation_df, '../data/quality_models/hat-models/hat-model-exp-act-encoder/', 'hatformer', {'exp_act_label': {'num_tokens':11, 'flow_model_hidden_size': 256, 'nhead':1, 'nlayers':3}})\n",
    "print('hat-enc-exp-act (predictions): ', hat_enc_exp_act_mse, hat_enc_exp_act_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Evalauting model ../data/quality_models/longformer-models/longformer-model-exp-act-encoder/ ------\n",
      "----- testing fold 0 ------\n",
      "Loading model: ../data/quality_models/longformer-models/longformer-model-exp-act-encoder//0-fold/\n",
      "Extra encoders:  ModuleList(\n",
      "  (0): MyTransformerEncoder(\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (5): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embedding): Embedding(11, 128)\n",
      "  )\n",
      ")\n",
      "Using input_clm=input_texts\n",
      "adding exp_act_label to the flows\n",
      "----- testing fold 1 ------\n",
      "Loading model: ../data/quality_models/longformer-models/longformer-model-exp-act-encoder//1-fold/\n",
      "Extra encoders:  ModuleList(\n",
      "  (0): MyTransformerEncoder(\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (5): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embedding): Embedding(11, 128)\n",
      "  )\n",
      ")\n",
      "Using input_clm=input_texts\n",
      "adding exp_act_label to the flows\n",
      "----- testing fold 2 ------\n",
      "Loading model: ../data/quality_models/longformer-models/longformer-model-exp-act-encoder//2-fold/\n",
      "Extra encoders:  ModuleList(\n",
      "  (0): MyTransformerEncoder(\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (5): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embedding): Embedding(11, 128)\n",
      "  )\n",
      ")\n",
      "Using input_clm=input_texts\n",
      "adding exp_act_label to the flows\n",
      "----- testing fold 3 ------\n",
      "Loading model: ../data/quality_models/longformer-models/longformer-model-exp-act-encoder//3-fold/\n",
      "Extra encoders:  ModuleList(\n",
      "  (0): MyTransformerEncoder(\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (5): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embedding): Embedding(11, 128)\n",
      "  )\n",
      ")\n",
      "Using input_clm=input_texts\n",
      "adding exp_act_label to the flows\n",
      "----- testing fold 4 ------\n",
      "Loading model: ../data/quality_models/longformer-models/longformer-model-exp-act-encoder//4-fold/\n",
      "Extra encoders:  ModuleList(\n",
      "  (0): MyTransformerEncoder(\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (1): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (2): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (3): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (4): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "        (5): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embedding): Embedding(11, 128)\n",
      "  )\n",
      ")\n",
      "Using input_clm=input_texts\n",
      "adding exp_act_label to the flows\n",
      "[1.1486931184359532, 1.2899962737441586, 1.2219037392450782, 1.4235692705651142, 1.1834774617324386]\n",
      "longformer-enc-exp-act (predictions):  1.2535279727445485 0.9697158387532226\n"
     ]
    }
   ],
   "source": [
    "longformer_enc_exp_act_mse, longformer_enc_exp_act_mae, _, _ = evaluate_model(eli5_annotation_df, '../data/quality_models/longformer-models/longformer-model-exp-act-encoder/', 'longformer', {'exp_act_label': {'num_tokens':11, 'flow_model_hidden_size': 128, 'nhead':4, 'nlayers':6}})\n",
    "print('longformer-enc-exp-act (predictions): ', longformer_enc_exp_act_mse, longformer_enc_exp_act_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hat-enc-exp-act (predictions):  1.3879571245086566 1.0832064471683118\n",
    "- longformer-enc-exp-act (predictions):  1.2535279727445485 0.9697158387532226"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
